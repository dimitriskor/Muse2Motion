{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(f\"Current working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## example code for single subject (features from muse) synchronized with time log file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# === Load EEG data ===\n",
    "eeg_file = \"/Users/kyrusmama/Downloads/ref_prep_synchr_MAT_jiahui/Data_all/sub-10/mindMonitor_2025-04-30--14-23-31_10.csv\"\n",
    "df = pd.read_csv(eeg_file)\n",
    "df[\"Time\"] = pd.to_datetime(df[\"TimeStamp\"])\n",
    "df.set_index(\"Time\", inplace=True)\n",
    "\n",
    "# === Load sync log file ===\n",
    "log_file = \"/Users/kyrusmama/Downloads/ref_prep_synchr_MAT_jiahui/Data_all/sub-10/eeg_experiment_timeline_log_10.txt\"\n",
    "sync_df = pd.read_csv(log_file, sep='\\t', header=None, names=[\"Event\", \"Time\"])\n",
    "sync_df = sync_df.dropna()\n",
    "\n",
    "# === Extract experiment start time and fix offset ===\n",
    "exp_start_event = sync_df[sync_df[\"Event\"].str.contains(\"Experiment Unix Start\")]\n",
    "experiment_unix_start = float(exp_start_event[\"Time\"].values[0]) + 2 * 3600  # Adjust 2-hour offset\n",
    "\n",
    "# === Compute absolute timestamps for task/rest intervals ===\n",
    "task_intervals, rest_intervals = [], []\n",
    "for i in range(len(sync_df) - 1):\n",
    "    event = sync_df.iloc[i][\"Event\"]\n",
    "    time_offset = sync_df.iloc[i][\"Time\"]\n",
    "    abs_time = pd.to_datetime(experiment_unix_start + time_offset, unit=\"s\")\n",
    "\n",
    "    if \"Task Start\" in event:\n",
    "        task_start = abs_time\n",
    "        task_end = pd.to_datetime(experiment_unix_start + sync_df.iloc[i + 1][\"Time\"], unit=\"s\")\n",
    "        task_intervals.append((task_start, task_end))\n",
    "\n",
    "    if \"Rest Start\" in event:\n",
    "        rest_start = abs_time\n",
    "        rest_end = pd.to_datetime(experiment_unix_start + sync_df.iloc[i + 1][\"Time\"], unit=\"s\")\n",
    "        rest_intervals.append((rest_start, rest_end))\n",
    "\n",
    "# === Compute average EEG band power across channels ===\n",
    "bands = [\"Delta\", \"Theta\", \"Alpha\", \"Beta\", \"Gamma\"]\n",
    "channels = [\"TP9\", \"AF7\", \"AF8\", \"TP10\"]\n",
    "band_cols = [f\"{band}_{ch}\" for band in bands for ch in channels if f\"{band}_{ch}\" in df.columns]\n",
    "\n",
    "df_band_avg = df[band_cols].copy()\n",
    "for band in bands:\n",
    "    cols = [f\"{band}_{ch}\" for ch in channels if f\"{band}_{ch}\" in df_band_avg.columns]\n",
    "    df_band_avg[band] = df_band_avg[cols].mean(axis=1).abs()\n",
    "\n",
    "# === Label all time windows: Baseline, Task, Rest, Music ===\n",
    "df_band_avg[\"Condition\"] = \"Unlabeled\"\n",
    "\n",
    "# Baseline\n",
    "baseline_start = pd.to_datetime(experiment_unix_start + sync_df[sync_df[\"Event\"] == \"Baseline Start\"][\"Time\"].values[0], unit=\"s\")\n",
    "baseline_end = pd.to_datetime(experiment_unix_start + sync_df[sync_df[\"Event\"] == \"Baseline End\"][\"Time\"].values[0], unit=\"s\")\n",
    "df_band_avg.loc[(df_band_avg.index >= baseline_start) & (df_band_avg.index <= baseline_end), \"Condition\"] = \"Baseline\"\n",
    "\n",
    "# Task and Rest\n",
    "for start, end in task_intervals:\n",
    "    df_band_avg.loc[(df_band_avg.index >= start) & (df_band_avg.index <= end), \"Condition\"] = \"Task\"\n",
    "\n",
    "for start, end in rest_intervals:\n",
    "    df_band_avg.loc[(df_band_avg.index >= start) & (df_band_avg.index <= end), \"Condition\"] = \"Rest\"\n",
    "\n",
    "# Music\n",
    "if \"Music Start\" in sync_df[\"Event\"].values and \"Music End\" in sync_df[\"Event\"].values:\n",
    "    music_start = pd.to_datetime(experiment_unix_start + sync_df[sync_df[\"Event\"] == \"Music Start\"][\"Time\"].values[0], unit=\"s\")\n",
    "    music_end = pd.to_datetime(experiment_unix_start + sync_df[sync_df[\"Event\"] == \"Music End\"][\"Time\"].values[0], unit=\"s\")\n",
    "    df_band_avg.loc[(df_band_avg.index >= music_start) & (df_band_avg.index <= music_end), \"Condition\"] = \"Music\"\n",
    "\n",
    "# Remove any remaining unlabeled data\n",
    "df_band_avg = df_band_avg[df_band_avg[\"Condition\"] != \"Unlabeled\"]\n",
    "\n",
    "# === Plot mean EEG band power by condition ===\n",
    "summary = df_band_avg.groupby(\"Condition\")[bands].mean()\n",
    "summary.T.plot(kind=\"bar\", colormap=\"Set2\", figsize=(10, 5))\n",
    "plt.title(\"Mean EEG Band Power by Condition\")\n",
    "plt.ylabel(\"Power\")\n",
    "plt.xlabel(\"EEG Band\")\n",
    "plt.xticks(rotation=0)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# === File paths ===\n",
    "eeg_file = \"/Users/kyrusmama/Downloads/ref_prep_synchr_MAT_jiahui/Data_all/sub-10/mindMonitor_2025-04-30--14-23-31_10.csv\"\n",
    "log_file = \"/Users/kyrusmama/Downloads/ref_prep_synchr_MAT_jiahui/Data_all/sub-10/eeg_experiment_timeline_log_10.txt\"\n",
    "\n",
    "# === 1) Load EEG data ===\n",
    "df = pd.read_csv(eeg_file)\n",
    "\n",
    "# --- Drop any Muse “event” rows so they don’t sneak into our band-power time series ---\n",
    "# these rows will have a timestamp and an Elements string, but no numeric data\n",
    "if \"Elements\" in df.columns:\n",
    "    df = df[df[\"Elements\"].isna()]\n",
    "\n",
    "# parse timestamps and set index\n",
    "df[\"Time\"] = pd.to_datetime(df[\"TimeStamp\"])\n",
    "df.set_index(\"Time\", inplace=True)\n",
    "\n",
    "# === 2) Load sync log and compute experiment start ===\n",
    "sync_df = pd.read_csv(log_file, sep=\"\\t\", header=None, names=[\"Event\", \"Time\"])\n",
    "sync_df.dropna(inplace=True)\n",
    "exp0 = float(sync_df.loc[sync_df[\"Event\"].str.contains(\"Experiment Unix Start\"), \"Time\"].iat[0])\n",
    "experiment_unix_start = exp0 + 2 * 3600  # apply your 2h offset\n",
    "to_ts = lambda offset: pd.to_datetime(experiment_unix_start + offset, unit=\"s\")\n",
    "\n",
    "# === 3) Build condition intervals ===\n",
    "intervals = {\"Baseline\": [], \"Task\": [], \"Rest\": [], \"Music\": []}\n",
    "# Baseline\n",
    "b0 = float(sync_df.loc[sync_df[\"Event\"]==\"Baseline Start\", \"Time\"].iat[0])\n",
    "b1 = float(sync_df.loc[sync_df[\"Event\"]==\"Baseline End\",   \"Time\"].iat[0])\n",
    "intervals[\"Baseline\"].append((to_ts(b0), to_ts(b1)))\n",
    "# Task & Rest\n",
    "for i, row in sync_df.iterrows():\n",
    "    ev, t0 = row[\"Event\"], float(row[\"Time\"])\n",
    "    if \"Task Start\" in ev:\n",
    "        t1 = float(sync_df.at[i+1, \"Time\"])\n",
    "        intervals[\"Task\"].append((to_ts(t0), to_ts(t1)))\n",
    "    if \"Rest Start\" in ev:\n",
    "        t1 = float(sync_df.at[i+1, \"Time\"])\n",
    "        intervals[\"Rest\"].append((to_ts(t0), to_ts(t1)))\n",
    "# Music (if present)\n",
    "if \"Music Start\" in sync_df[\"Event\"].values:\n",
    "    m0 = float(sync_df.loc[sync_df[\"Event\"]==\"Music Start\", \"Time\"].iat[0])\n",
    "    m1 = float(sync_df.loc[sync_df[\"Event\"]==\"Music End\",   \"Time\"].iat[0])\n",
    "    intervals[\"Music\"].append((to_ts(m0), to_ts(m1)))\n",
    "\n",
    "# === 4) Plot each Alpha channel separately ===\n",
    "alpha_cols = [c for c in df.columns if c.startswith(\"Alpha_\")]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "# plot each channel\n",
    "for ch in alpha_cols:\n",
    "    ax.plot(df.index, df[ch].abs(), label=ch, linewidth=1)\n",
    "\n",
    "# background shading\n",
    "colors = {\n",
    "    \"Baseline\": \"#DDDDDD\",\n",
    "    \"Task\":     \"#FFDDDD\",\n",
    "    \"Rest\":     \"#DDFFDD\",\n",
    "    \"Music\":    \"#DDDDFF\",\n",
    "}\n",
    "for cond, spans in intervals.items():\n",
    "    for start, end in spans:\n",
    "        ax.axvspan(start, end, color=colors[cond], alpha=0.3, label=cond)\n",
    "\n",
    "# clean legend (no duplicate condition labels)\n",
    "lines, labels = ax.get_legend_handles_labels()\n",
    "seen = set()\n",
    "ordered = []\n",
    "for ln, lbl in zip(lines, labels):\n",
    "    if lbl in seen:\n",
    "        continue\n",
    "    ordered.append((ln, lbl))\n",
    "    seen.add(lbl)\n",
    "ax.legend([ln for ln, _ in ordered], [lbl for _, lbl in ordered],\n",
    "          loc=\"upper right\", fontsize=\"small\")\n",
    "\n",
    "ax.set_title(\"Alpha Band Power by Channel Over Time\")\n",
    "ax.set_xlabel(\"Time\")\n",
    "ax.set_ylabel(\"Absolute Alpha Power\")\n",
    "ax.grid(True)\n",
    "fig.autofmt_xdate()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# === File paths ===\n",
    "# === File paths ===\n",
    "eeg_file = \"/Users/kyrusmama/Downloads/ref_prep_synchr_MAT_jiahui/Data_all/sub-10/mindMonitor_2025-04-30--14-23-31_10.csv\"\n",
    "log_file = \"/Users/kyrusmama/Downloads/ref_prep_synchr_MAT_jiahui/Data_all/sub-10/eeg_experiment_timeline_log_10.txt\"\n",
    "\n",
    "# === 1) Load & clean EEG data ===\n",
    "df = pd.read_csv(eeg_file)\n",
    "\n",
    "# drop any Muse “event” lines (blinks, jaw clenches, etc.)\n",
    "if \"Elements\" in df.columns:\n",
    "    df = df[df[\"Elements\"].isna()]\n",
    "\n",
    "# parse timestamps\n",
    "df[\"Time\"] = pd.to_datetime(df[\"TimeStamp\"])\n",
    "df.set_index(\"Time\", inplace=True)\n",
    "\n",
    "# === 2) Load sync log & build condition intervals ===\n",
    "sync_df = pd.read_csv(log_file, sep=\"\\t\", header=None, names=[\"Event\", \"Time\"])\n",
    "sync_df.dropna(inplace=True)\n",
    "\n",
    "# find experiment unix start + your 2 h offset\n",
    "exp0 = float(sync_df.loc[sync_df[\"Event\"].str.contains(\"Experiment Unix Start\"), \"Time\"].iat[0])\n",
    "t0 = exp0 + 2 * 3600\n",
    "to_ts = lambda offs: pd.to_datetime(t0 + float(offs), unit=\"s\")\n",
    "\n",
    "intervals = {\"Baseline\": [], \"Task\": [], \"Rest\": [], \"Music\": []}\n",
    "\n",
    "# baseline\n",
    "b0 = sync_df.loc[sync_df[\"Event\"]==\"Baseline Start\", \"Time\"].iat[0]\n",
    "b1 = sync_df.loc[sync_df[\"Event\"]==\"Baseline End\",   \"Time\"].iat[0]\n",
    "intervals[\"Baseline\"].append((to_ts(b0), to_ts(b1)))\n",
    "\n",
    "# task & rest\n",
    "for i, row in sync_df.iterrows():\n",
    "    ev, offs = row[\"Event\"], row[\"Time\"]\n",
    "    if \"Task Start\" in ev:\n",
    "        intervals[\"Task\"].append((to_ts(offs), to_ts(sync_df.at[i+1,\"Time\"])))\n",
    "    if \"Rest Start\" in ev:\n",
    "        intervals[\"Rest\"].append((to_ts(offs), to_ts(sync_df.at[i+1,\"Time\"])))\n",
    "\n",
    "# music (if present)\n",
    "if \"Music Start\" in sync_df[\"Event\"].values:\n",
    "    m0 = sync_df.loc[sync_df[\"Event\"]==\"Music Start\", \"Time\"].iat[0]\n",
    "    m1 = sync_df.loc[sync_df[\"Event\"]==\"Music End\",   \"Time\"].iat[0]\n",
    "    intervals[\"Music\"].append((to_ts(m0), to_ts(m1)))\n",
    "\n",
    "# === 3) Label each row with its Condition ===\n",
    "df[\"Condition\"] = \"Unlabeled\"\n",
    "for cond, spans in intervals.items():\n",
    "    for start, end in spans:\n",
    "        mask = (df.index >= start) & (df.index <= end)\n",
    "        df.loc[mask, \"Condition\"] = cond\n",
    "\n",
    "# drop any unlabeled samples\n",
    "df = df[df[\"Condition\"] != \"Unlabeled\"]\n",
    "\n",
    "# === 4) Build arrays ===\n",
    "\n",
    "# 4a) feature matrix: features × time\n",
    "#    select all numeric columns (drops TimeStamp, Elements, Condition)\n",
    "feature_cols = df.select_dtypes(include=\"number\").columns.tolist()\n",
    "X = df[feature_cols].T.values      # shape = (n_features, n_timesteps)\n",
    "\n",
    "# 4b) labels per timepoint\n",
    "y = df[\"Condition\"].values         # shape = (n_timesteps,)\n",
    "\n",
    "# 4c) actual timestamps\n",
    "times = df.index.to_numpy()        # shape = (n_timesteps,)\n",
    "\n",
    "# === 5) Save to disk ===\n",
    "np.save(\"features.npy\", X)\n",
    "np.save(\"labels.npy\",   y)\n",
    "np.save(\"times.npy\",    times)\n",
    "\n",
    "print(f\"Saved:\\n - features.npy: {X.shape}\\n - labels.npy:   {y.shape}\\n - times.npy:    {times.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from hmmlearn import hmm\n",
    "from collections import Counter\n",
    "import itertools\n",
    "\n",
    "# --- 1) Load your arrays (or replace these with your DataFrame extraction) ---\n",
    "# X: np.ndarray, shape (n_features,  n_timesteps)\n",
    "# y: np.ndarray, shape (n_timesteps,) of strings like \"Baseline\",\"Task\",...\n",
    "# times: np.ndarray, shape (n_timesteps,) of datetime64\n",
    "\n",
    "X     = X\n",
    "y_true = y      # (n_t,)\n",
    "times = times              # (n_t,)\n",
    "\n",
    "# --- 2) Encode states as integers ---\n",
    "states = sorted(np.unique(y_true))\n",
    "n_states = len(states)\n",
    "state_to_idx = {s:i for i,s in enumerate(states)}\n",
    "idx_to_state = {i:s for s,i in state_to_idx.items()}\n",
    "y_idx = np.array([state_to_idx[s] for s in y_true])\n",
    "\n",
    "# --- 3) Estimate π and A from the true labels ---\n",
    "# initial distribution: 1 at the very first true state\n",
    "pi = np.zeros(n_states)\n",
    "pi[y_idx[0]] = 1.0\n",
    "\n",
    "# transition matrix counts\n",
    "A_counts = np.zeros((n_states, n_states))\n",
    "for (i,j) in zip(y_idx[:-1], y_idx[1:]):\n",
    "    A_counts[i,j] += 1\n",
    "# normalize\n",
    "A = (A_counts.T / A_counts.sum(axis=1)).T\n",
    "# fix any rows that sum to zero\n",
    "for i in range(n_states):\n",
    "    if A[i].sum() == 0:\n",
    "        A[i,i] = 1.0\n",
    "\n",
    "# --- 4) Fit one Gaussian per state on its observations ---\n",
    "# we'll fit full‐covariance Gaussians\n",
    "means   = []\n",
    "covars  = []\n",
    "reg = 1e-2   # try 1e-2 or even 1e-1 if needed\n",
    "\n",
    "for i in range(n_states):\n",
    "    obs = X[:, y_idx == i].T\n",
    "    mu = obs.mean(axis=0)\n",
    "    # empirical covariance\n",
    "    C = np.cov(obs, rowvar=False)\n",
    "    # symmetrize\n",
    "    C = 0.5 * (C + C.T)\n",
    "    # add jitter\n",
    "    C += reg * np.eye(C.shape[0])\n",
    "\n",
    "    means.append(mu)\n",
    "    covars.append(C)\n",
    "\n",
    "means  = np.vstack(means)\n",
    "covars = np.stack(covars)\n",
    "# --- 5) Build and decode with the HMM ---\n",
    "model = hmm.GaussianHMM(n_components=n_states,\n",
    "                        covariance_type=\"full\", \n",
    "                        init_params=\"\")  \n",
    "# we disable init_params so we can set our own π, A, means, covars\n",
    "model.startprob_ = pi\n",
    "model.transmat_  = A\n",
    "model.means_     = means\n",
    "model.covars_    = covars\n",
    "\n",
    "# Viterbi decode (we need shape (n_t, n_features))\n",
    "logprob, y_pred_idx = model.decode(X.T, algorithm=\"viterbi\")\n",
    "y_pred = np.array([idx_to_state[i] for i in y_pred_idx])\n",
    "\n",
    "# --- 6) Compute accuracy ---\n",
    "accuracy = (y_pred == y_true).mean()\n",
    "print(f\"Viterbi decoding accuracy: {accuracy*100:.2f}%\")\n",
    "\n",
    "# --- 7) Plot predicted state over time ---\n",
    "# assign each state a y‐value and color\n",
    "state_y     = {s:i for i,s in enumerate(states)}\n",
    "state_color = dict(zip(states, plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"][:n_states]))\n",
    "\n",
    "y_pred_y = [state_y[s] for s in y_pred]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,3))\n",
    "ax.step(times, y_pred_y, where=\"post\", lw=1.5)\n",
    "ax.set_yticks(list(state_y.values()))\n",
    "ax.set_yticklabels(states)\n",
    "ax.set_xlabel(\"Time\")\n",
    "ax.set_title(\"Viterbi‐Predicted Condition Over Time\")\n",
    "ax.grid(axis=\"x\", linestyle=\"--\", alpha=0.4)\n",
    "\n",
    "# shade background by predicted state\n",
    "for i, (t0, t1) in enumerate(zip(times[:-1], times[1:])):\n",
    "    c = state_color[y_pred[i]]\n",
    "    ax.axvspan(t0, t1, color=c, alpha=0.1)\n",
    "\n",
    "fig.autofmt_xdate()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from hmmlearn import hmm\n",
    "\n",
    "\n",
    "X     = X\n",
    "y_true = y      # (n_t,)\n",
    "times = times              # (n_t,)\n",
    "\n",
    "# --- 2) (Re)load feature names so we can label the PDF plot ---\n",
    "# adjust this path to your actual CSV\n",
    "eeg_file = \"/Users/kyrusmama/Downloads/ref_prep_synchr_MAT_jiahui/Data_all/sub-10/mindMonitor_2025-04-30--14-23-31_10.csv\"\n",
    "df_full   = pd.read_csv(eeg_file)\n",
    "if \"Elements\" in df_full.columns:\n",
    "    df_full = df_full[df_full[\"Elements\"].isna()]\n",
    "df_full[\"Time\"] = pd.to_datetime(df_full[\"TimeStamp\"])\n",
    "df_full.set_index(\"Time\", inplace=True)\n",
    "feature_cols = df_full.select_dtypes(include=\"number\").columns.tolist()\n",
    "\n",
    "# --- 3) Re‐run the HMM setup & Viterbi (as before) ---\n",
    "states     = sorted(np.unique(y_true))\n",
    "n_states   = len(states)\n",
    "state2idx  = {s:i for i,s in enumerate(states)}\n",
    "idx2state  = {i:s for s,i in state2idx.items()}\n",
    "y_idx      = np.array([state2idx[s] for s in y_true])\n",
    "\n",
    "# initial π\n",
    "pi = np.zeros(n_states); pi[y_idx[0]] = 1.0\n",
    "\n",
    "# transition counts→matrix\n",
    "A_counts = np.zeros((n_states,n_states))\n",
    "for i,j in zip(y_idx[:-1], y_idx[1:]):\n",
    "    A_counts[i,j] += 1\n",
    "A = (A_counts.T / A_counts.sum(axis=1)).T\n",
    "for i in range(n_states):\n",
    "    if A[i].sum()==0: A[i,i]=1.0\n",
    "\n",
    "# fit one Gaussian per state\n",
    "means, covars = [], []\n",
    "reg = 1e-2\n",
    "for i in range(n_states):\n",
    "    obs = X[:, y_idx==i].T\n",
    "    mu  = obs.mean(0)\n",
    "    C   = np.cov(obs, rowvar=False)\n",
    "    C   = 0.5*(C + C.T) + reg*np.eye(C.shape[0])\n",
    "    means.append(mu); covars.append(C)\n",
    "means  = np.vstack(means)\n",
    "covars = np.stack(covars)\n",
    "\n",
    "# build HMM & decode\n",
    "model = hmm.GaussianHMM(n_components=n_states, covariance_type=\"full\", init_params=\"\")\n",
    "model.startprob_ = pi\n",
    "model.transmat_  = A\n",
    "model.means_     = means\n",
    "model.covars_    = covars\n",
    "\n",
    "_, y_pred_idx = model.decode(X.T, algorithm=\"viterbi\")\n",
    "y_pred = np.array([idx2state[i] for i in y_pred_idx])\n",
    "\n",
    "# accuracy\n",
    "acc = (y_pred==y_true).mean()\n",
    "print(f\"Decoding accuracy: {acc*100:.1f}%\")\n",
    "\n",
    "# --- 4) Plot true vs predicted states over time ---\n",
    "state2y = {s:i for i,s in enumerate(states)}\n",
    "y_true_int = [state2y[s] for s in y_true]\n",
    "y_pred_int = [state2y[s] for s in y_pred]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,4))\n",
    "ax.step(times, y_true_int, where=\"post\", linestyle=\"--\", label=\"True\",   lw=1)\n",
    "ax.step(times, y_pred_int, where=\"post\", linestyle=\"-\",  label=\"Predicted\", lw=1.5)\n",
    "\n",
    "ax.set_yticks(list(state2y.values()))\n",
    "ax.set_yticklabels(states)\n",
    "ax.set_xlabel(\"Time\")\n",
    "ax.set_title(\"True vs. Viterbi‐Predicted Condition\")\n",
    "ax.legend(loc=\"upper right\", fontsize=\"small\")\n",
    "ax.grid(axis=\"x\", linestyle=\"--\", alpha=0.3)\n",
    "fig.autofmt_xdate()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- 5) Plot emission PDFs for an example feature ---\n",
    "feat_idx  = -1                     # e.g. first feature in feature_cols\n",
    "feat_name = feature_cols[feat_idx]\n",
    "\n",
    "# value grid\n",
    "vals  = X[feat_idx, :]\n",
    "xgrid = np.linspace(vals.min(), vals.max(), 200)\n",
    "\n",
    "fig2, ax2 = plt.subplots(figsize=(8,4))\n",
    "for i, state in enumerate(states):\n",
    "    mu   = means[i, feat_idx]\n",
    "    var  = covars[i, feat_idx, feat_idx]\n",
    "    pdf  = (1/np.sqrt(2*np.pi*var)) * np.exp(-0.5*((xgrid-mu)**2/var))\n",
    "    ax2.plot(xgrid, pdf, label=state)\n",
    "\n",
    "ax2.set_xlabel(feat_name)\n",
    "ax2.set_ylabel(\"Probability Density\")\n",
    "ax2.set_title(f\"Emission PDFs for “{feat_name}”\")\n",
    "ax2.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from hmmlearn import hmm\n",
    "\n",
    "# === User‐configurable file paths ===\n",
    "eeg_file = \"/Users/kyrusmama/Downloads/ref_prep_synchr_MAT_jiahui/Data_all/sub-10/mindMonitor_2025-04-30--14-23-31_10.csv\"\n",
    "log_file = \"/Users/kyrusmama/Downloads/ref_prep_synchr_MAT_jiahui/Data_all/sub-10/eeg_experiment_timeline_log_10.txt\"\n",
    "\n",
    "# === 1) Load & clean EEG data ===\n",
    "df = pd.read_csv(eeg_file)\n",
    "\n",
    "# Drop any Muse “event” rows (blinks, jaw clenches, etc.)\n",
    "if \"Elements\" in df.columns:\n",
    "    df = df[df[\"Elements\"].isna()]\n",
    "\n",
    "# Parse timestamps\n",
    "df[\"Time\"] = pd.to_datetime(df[\"TimeStamp\"])\n",
    "df.set_index(\"Time\", inplace=True)\n",
    "\n",
    "# === 2) Load sync log & compute absolute condition intervals ===\n",
    "sync_df = pd.read_csv(log_file, sep=\"\\t\", header=None, names=[\"Event\",\"Time\"])\n",
    "sync_df.dropna(inplace=True)\n",
    "\n",
    "# Find experiment start (Unix) and apply 2 h offset\n",
    "exp0 = float(sync_df.loc[sync_df[\"Event\"].str.contains(\"Experiment Unix Start\"), \"Time\"].iat[0])\n",
    "t0  = exp0 + 2 * 3600\n",
    "to_ts = lambda offs: pd.to_datetime(t0 + float(offs), unit=\"s\")\n",
    "\n",
    "# Build intervals dict\n",
    "intervals = {\"Baseline\": [], \"Task\": [], \"Rest\": [], \"Music\": []}\n",
    "\n",
    "# Baseline\n",
    "b0 = sync_df.loc[sync_df[\"Event\"]==\"Baseline Start\", \"Time\"].iat[0]\n",
    "b1 = sync_df.loc[sync_df[\"Event\"]==\"Baseline End\",   \"Time\"].iat[0]\n",
    "intervals[\"Baseline\"].append((to_ts(b0), to_ts(b1)))\n",
    "\n",
    "# Task & Rest (paired starts → next event)\n",
    "for i, row in sync_df.iterrows():\n",
    "    ev, offs = row[\"Event\"], row[\"Time\"]\n",
    "    if \"Task Start\" in ev:\n",
    "        intervals[\"Task\"].append((to_ts(offs), to_ts(sync_df.at[i+1,\"Time\"])))\n",
    "    if \"Rest Start\" in ev:\n",
    "        intervals[\"Rest\"].append((to_ts(offs), to_ts(sync_df.at[i+1,\"Time\"])))\n",
    "\n",
    "# Music (if present)\n",
    "if \"Music Start\" in sync_df[\"Event\"].values:\n",
    "    m0 = sync_df.loc[sync_df[\"Event\"]==\"Music Start\", \"Time\"].iat[0]\n",
    "    m1 = sync_df.loc[sync_df[\"Event\"]==\"Music End\",   \"Time\"].iat[0]\n",
    "    intervals[\"Music\"].append((to_ts(m0), to_ts(m1)))\n",
    "\n",
    "# === 3) Label each EEG sample with its condition ===\n",
    "df[\"Condition\"] = \"Unlabeled\"\n",
    "for cond, spans in intervals.items():\n",
    "    for start, end in spans:\n",
    "        mask = (df.index >= start) & (df.index <= end)\n",
    "        df.loc[mask, \"Condition\"] = cond\n",
    "\n",
    "# Remove unlabeled samples\n",
    "df = df[df[\"Condition\"] != \"Unlabeled\"]\n",
    "\n",
    "# === 4) Build feature/time/label arrays ===\n",
    "# 4a) Features: all numeric columns except TimeStamp → shape (n_features, n_t)\n",
    "feature_cols = df.select_dtypes(include=\"number\").columns.drop(\n",
    "    [\"TimeStamp\"] if \"TimeStamp\" in df.columns else []\n",
    ").tolist()\n",
    "X = df[feature_cols].T.values\n",
    "\n",
    "# 4b) True labels\n",
    "y_true = df[\"Condition\"].values\n",
    "\n",
    "# 4c) Timestamps\n",
    "times = df.index.to_numpy()\n",
    "\n",
    "# === 5) Encode states & estimate HMM parameters ===\n",
    "states     = sorted(np.unique(y_true))\n",
    "state2idx  = {s:i for i,s in enumerate(states)}\n",
    "idx2state  = {i:s for s,i in state2idx.items()}\n",
    "y_idx      = np.array([state2idx[s] for s in y_true])\n",
    "n_states   = len(states)\n",
    "\n",
    "# Initial distribution π\n",
    "pi = np.zeros(n_states)\n",
    "pi[y_idx[0]] = 1.0\n",
    "\n",
    "# Transition matrix A\n",
    "A_counts = np.zeros((n_states, n_states))\n",
    "for a, b in zip(y_idx[:-1], y_idx[1:]):\n",
    "    A_counts[a,b] += 1\n",
    "A = (A_counts.T / A_counts.sum(axis=1)).T\n",
    "# Ensure no zero‐rows\n",
    "for i in range(n_states):\n",
    "    if A[i].sum() == 0:\n",
    "        A[i,i] = 1.0\n",
    "\n",
    "# Emission Gaussians: one per state\n",
    "means, covars = [], []\n",
    "reg = 1e-2  # jitter for PD\n",
    "for i in range(n_states):\n",
    "    obs = X[:, y_idx==i].T\n",
    "    mu  = obs.mean(axis=0)\n",
    "    C   = np.cov(obs, rowvar=False)\n",
    "    C   = 0.5*(C + C.T) + reg*np.eye(C.shape[1])\n",
    "    means.append(mu)\n",
    "    covars.append(C)\n",
    "means  = np.vstack(means)\n",
    "covars = np.stack(covars)\n",
    "\n",
    "# === 6) Build HMM, decode via Viterbi ===\n",
    "model = hmm.GaussianHMM(n_components=n_states,\n",
    "                        covariance_type=\"full\",\n",
    "                        init_params=\"\")\n",
    "model.startprob_ = pi\n",
    "model.transmat_  = A\n",
    "model.means_     = means\n",
    "model.covars_    = covars\n",
    "\n",
    "_, y_pred_idx = model.decode(X.T, algorithm=\"viterbi\")\n",
    "y_pred = np.array([idx2state[i] for i in y_pred_idx])\n",
    "\n",
    "# Compute accuracy\n",
    "accuracy = (y_pred == y_true).mean()\n",
    "print(f\"Viterbi decoding accuracy: {accuracy*100:.2f}%\")\n",
    "\n",
    "# === 7) Plot true vs. predicted over time ===\n",
    "state_y   = {s:i for i,s in enumerate(states)}\n",
    "y_true_i  = [state_y[s] for s in y_true]\n",
    "y_pred_i  = [state_y[s] for s in y_pred]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,4))\n",
    "ax.step(times, y_true_i, where=\"post\", linestyle=\"--\", label=\"True\",   lw=1)\n",
    "ax.step(times, y_pred_i, where=\"post\", linestyle=\"-\",  label=\"Predicted\", lw=1.5)\n",
    "ax.set_yticks(list(state_y.values()))\n",
    "ax.set_yticklabels(states)\n",
    "ax.set_xlabel(\"Time\")\n",
    "ax.set_title(\"True vs. Viterbi‐Predicted Experimental Condition\")\n",
    "ax.legend(loc=\"upper right\", fontsize=\"small\")\n",
    "ax.grid(axis=\"x\", linestyle=\"--\", alpha=0.3)\n",
    "fig.autofmt_xdate()\n",
    "plt.tight_layout()\n",
    "\n",
    "# === 8) Plot example emission PDFs for the first feature ===\n",
    "feat_idx  = 0\n",
    "feat_name = feature_cols[feat_idx]\n",
    "vals      = X[feat_idx, :]\n",
    "xgrid     = np.linspace(vals.min(), vals.max(), 200)\n",
    "\n",
    "fig2, ax2 = plt.subplots(figsize=(8,4))\n",
    "for i, state in enumerate(states):\n",
    "    mu   = means[i, feat_idx]\n",
    "    var  = covars[i, feat_idx, feat_idx]\n",
    "    pdf  = (1/np.sqrt(2*np.pi*var)) * np.exp(-0.5*((xgrid-mu)**2/var))\n",
    "    ax2.plot(xgrid, pdf, label=state)\n",
    "ax2.set_xlabel(feat_name)\n",
    "ax2.set_ylabel(\"Probability Density\")\n",
    "ax2.set_title(f\"Emission PDFs for “{feat_name}”\")\n",
    "ax2.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from hmmlearn import hmm\n",
    "\n",
    "# === User‐configurable file paths ===\n",
    "# eeg_file = \"/Users/kyrusmama/Downloads/ref_prep_synchr_MAT_jiahui/Data_all/sub-10/mindMonitor_2025-04-30--14-23-31_10.csv\"\n",
    "# log_file = \"/Users/kyrusmama/Downloads/ref_prep_synchr_MAT_jiahui/Data_all/sub-10/eeg_experiment_timeline_log_10.txt\"\n",
    "\n",
    "# === User‐configurable file paths ===\n",
    "# eeg_file = \"/Users/kyrusmama/Downloads/ref_prep_synchr_MAT_jiahui/Data_all/sub-9/mindMonitor_2025-04-30--13-53-32_9.csv\"\n",
    "# log_file = \"/Users/kyrusmama/Downloads/ref_prep_synchr_MAT_jiahui/Data_all/sub-9/eeg_experiment_timeline_log_9.txt\"\n",
    "\n",
    "# === User‐configurable file paths ===\n",
    "eeg_file = \"/Users/kyrusmama/Downloads/ref_prep_synchr_MAT_jiahui/Data_all/sub-8/mindMonitor_2025-04-29--22-41-21_8.csv\"\n",
    "log_file = \"/Users/kyrusmama/Downloads/ref_prep_synchr_MAT_jiahui/Data_all/sub-8/eeg_experiment_timeline_log_8.txt\"\n",
    "\n",
    "# List the columns you want to use as your observation features.\n",
    "# E.g. try [\"Alpha_TP9\",\"Alpha_TP10\"] or [\"Delta_TP9\",\"Theta_TP9\",\"Alpha_TP9\"], etc.\n",
    "# If this list is empty, the script will fall back to using all numeric columns.\n",
    "# selected_features = [\n",
    "#     \"Delta_TP9\", \"Theta_TP9\", \"Alpha_TP9\",\n",
    "#     \"Beta_TP9\", \"Gamma_TP9\"\n",
    "# ]\n",
    "\n",
    "selected_features = [\n",
    "  \"Delta_TP9\", \"Delta_AF7\", \"Delta_AF8\", \"Delta_TP10\",\n",
    "  \"Theta_TP9\", \"Theta_AF7\", \"Theta_AF8\", \"Theta_TP10\",\n",
    "  \"Alpha_TP9\", \"Alpha_AF7\", \"Alpha_AF8\", \"Alpha_TP10\",\n",
    "  \"Beta_TP9\",  \"Beta_AF7\",  \"Beta_AF8\",  \"Beta_TP10\",\n",
    "  \"Gamma_TP9\", \"Gamma_AF7\", \"Gamma_AF8\", \"Gamma_TP10\"\n",
    "]\n",
    "\n",
    "# === 1) Load & clean EEG data ===\n",
    "df = pd.read_csv(eeg_file)\n",
    "\n",
    "# drop any Muse “event” lines so they don't pollute features\n",
    "if \"Elements\" in df.columns:\n",
    "    df = df[df[\"Elements\"].isna()]\n",
    "\n",
    "# parse timestamps & index\n",
    "df[\"Time\"] = pd.to_datetime(df[\"TimeStamp\"])\n",
    "df.set_index(\"Time\", inplace=True)\n",
    "\n",
    "# === 2) Load sync log & build absolute condition intervals ===\n",
    "sync_df = pd.read_csv(log_file, sep=\"\\t\", header=None, names=[\"Event\",\"Time\"])\n",
    "sync_df.dropna(inplace=True)\n",
    "\n",
    "# find experiment unix start + 2h offset\n",
    "exp0 = float(sync_df.loc[sync_df[\"Event\"].str.contains(\"Experiment Unix Start\"), \"Time\"].iat[0])\n",
    "t0   = exp0 + 2*3600\n",
    "to_ts = lambda offs: pd.to_datetime(t0 + float(offs), unit=\"s\")\n",
    "\n",
    "intervals = {\"Baseline\": [], \"Task\": [], \"Rest\": [], \"Music\": []}\n",
    "\n",
    "# baseline\n",
    "b0 = sync_df.loc[sync_df[\"Event\"]==\"Baseline Start\", \"Time\"].iat[0]\n",
    "b1 = sync_df.loc[sync_df[\"Event\"]==\"Baseline End\",   \"Time\"].iat[0]\n",
    "intervals[\"Baseline\"].append((to_ts(b0), to_ts(b1)))\n",
    "\n",
    "# task & rest\n",
    "for i,row in sync_df.iterrows():\n",
    "    ev, offs = row[\"Event\"], row[\"Time\"]\n",
    "    if \"Task Start\" in ev:\n",
    "        intervals[\"Task\"].append((to_ts(offs), to_ts(sync_df.at[i+1,\"Time\"])))\n",
    "    if \"Rest Start\" in ev:\n",
    "        intervals[\"Rest\"].append((to_ts(offs), to_ts(sync_df.at[i+1,\"Time\"])))\n",
    "\n",
    "# music (optional)\n",
    "if \"Music Start\" in sync_df[\"Event\"].values:\n",
    "    m0 = sync_df.loc[sync_df[\"Event\"]==\"Music Start\", \"Time\"].iat[0]\n",
    "    m1 = sync_df.loc[sync_df[\"Event\"]==\"Music End\",   \"Time\"].iat[0]\n",
    "    intervals[\"Music\"].append((to_ts(m0), to_ts(m1)))\n",
    "\n",
    "# === 3) Label each sample with its condition ===\n",
    "df[\"Condition\"] = \"Unlabeled\"\n",
    "for cond, spans in intervals.items():\n",
    "    for start,end in spans:\n",
    "        df.loc[(df.index>=start)&(df.index<=end), \"Condition\"] = cond\n",
    "df = df[df[\"Condition\"]!=\"Unlabeled\"]  # drop any outside intervals\n",
    "\n",
    "# === 4) Select features & build arrays ===\n",
    "# determine which columns to use\n",
    "if selected_features:\n",
    "    missing = set(selected_features) - set(df.columns)\n",
    "    if missing:\n",
    "        print(f\"Warning: these selected_features were not found and will be skipped: {missing}\")\n",
    "    feature_cols = [c for c in selected_features if c in df.columns]\n",
    "else:\n",
    "    # fallback: all numeric columns (drops TimeStamp / Elements / Condition automatically)\n",
    "    feature_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "print(f\"Using {len(feature_cols)} features:\", feature_cols)\n",
    "\n",
    "# build X, y_true, times\n",
    "X      = df[feature_cols].T.values      # shape: (n_features, n_timesteps)\n",
    "y_true = df[\"Condition\"].values         # shape: (n_timesteps,)\n",
    "times  = df.index.to_numpy()            # shape: (n_timesteps,)\n",
    "\n",
    "# === 5) Encode states & estimate HMM params ===\n",
    "states    = sorted(np.unique(y_true))\n",
    "state2idx = {s:i for i,s in enumerate(states)}\n",
    "idx2state = {i:s for s,i in state2idx.items()}\n",
    "y_idx     = np.array([state2idx[s] for s in y_true])\n",
    "n_states  = len(states)\n",
    "\n",
    "# initial π (start in the first true label)\n",
    "pi = np.zeros(n_states); pi[y_idx[0]] = 1.0\n",
    "\n",
    "# transition counts → matrix\n",
    "A_counts = np.zeros((n_states,n_states))\n",
    "for a,b in zip(y_idx[:-1], y_idx[1:]):\n",
    "    A_counts[a,b] += 1\n",
    "A = (A_counts.T / A_counts.sum(axis=1)).T\n",
    "for i in range(n_states):\n",
    "    if A[i].sum()==0: A[i,i]=1.0\n",
    "\n",
    "# per‐state Gaussian emissions\n",
    "means, covars = [], []\n",
    "reg = 1e-2\n",
    "for i in range(n_states):\n",
    "    obs = X[:, y_idx==i].T\n",
    "    mu  = obs.mean(axis=0)\n",
    "    C   = np.cov(obs, rowvar=False)\n",
    "    C   = 0.5*(C + C.T) + reg*np.eye(C.shape[1])\n",
    "    means.append(mu)\n",
    "    covars.append(C)\n",
    "means  = np.vstack(means)\n",
    "covars = np.stack(covars)\n",
    "\n",
    "# === 6) Build HMM & Viterbi decode ===\n",
    "model = hmm.GaussianHMM(n_components=n_states,\n",
    "                        covariance_type=\"full\",\n",
    "                        init_params=\"\")\n",
    "model.startprob_ = pi\n",
    "model.transmat_  = A\n",
    "model.means_     = means\n",
    "model.covars_    = covars\n",
    "\n",
    "_, y_pred_idx = model.decode(X.T, algorithm=\"viterbi\")\n",
    "y_pred = np.array([idx2state[i] for i in y_pred_idx])\n",
    "\n",
    "# accuracy\n",
    "acc = (y_pred==y_true).mean()\n",
    "print(f\"Viterbi decoding accuracy: {acc*100:.2f}%\")\n",
    "\n",
    "# === 7) Plot true vs predicted states ===\n",
    "state_y    = {s:i for i,s in enumerate(states)}\n",
    "y_true_i   = [state_y[s] for s in y_true]\n",
    "y_pred_i   = [state_y[s] for s in y_pred]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,4))\n",
    "ax.step(times, y_true_i, where=\"post\", linestyle=\"--\", label=\"True\",   lw=1)\n",
    "ax.step(times, y_pred_i, where=\"post\", linestyle=\"-\",  label=\"Predicted\", lw=1.5)\n",
    "ax.set_yticks(list(state_y.values())); ax.set_yticklabels(states)\n",
    "ax.set_xlabel(\"Time\"); ax.set_title(\"True vs. Predicted Condition\")\n",
    "ax.legend(loc=\"upper right\", fontsize=\"small\"); ax.grid(axis=\"x\", linestyle=\"--\", alpha=0.3)\n",
    "fig.autofmt_xdate(); plt.tight_layout()\n",
    "\n",
    "# === 8) Plot example emission PDFs for the first selected feature ===\n",
    "if feature_cols:\n",
    "    feat_idx  = 1\n",
    "    feat_name = feature_cols[feat_idx]\n",
    "    vals      = X[feat_idx, :]\n",
    "    xgrid     = np.linspace(vals.min(), vals.max(), 200)\n",
    "\n",
    "    fig2, ax2 = plt.subplots(figsize=(8,4))\n",
    "    for i,state in enumerate(states):\n",
    "        mu  = means[i, feat_idx]\n",
    "        var = covars[i, feat_idx, feat_idx]\n",
    "        pdf = 1/np.sqrt(2*np.pi*var) * np.exp(-0.5*((xgrid-mu)**2/var))\n",
    "        ax2.plot(xgrid, pdf, label=state)\n",
    "    ax2.set_xlabel(feat_name)\n",
    "    ax2.set_ylabel(\"Density\")\n",
    "    ax2.set_title(f\"Emission PDFs for “{feat_name}”\")\n",
    "    ax2.legend()\n",
    "    plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from hmmlearn import hmm\n",
    "\n",
    "# === User settings ===\n",
    "root_dir = \"/Users/kyrusmama/Downloads/ref_prep_synchr_MAT_jiahui/Data_all\"  # directory containing sub-1, sub-2, etc.\n",
    "# If you want to try specific features, list them here; leave empty to use all numeric columns:\n",
    "selected_features = [\n",
    "  \"Delta_TP9\", \"Delta_AF7\", \"Delta_AF8\", \"Delta_TP10\",\n",
    "  \"Theta_TP9\", \"Theta_AF7\", \"Theta_AF8\", \"Theta_TP10\",\n",
    "  \"Alpha_TP9\", \"Alpha_AF7\", \"Alpha_AF8\", \"Alpha_TP10\",\n",
    "  \"Beta_TP9\",  \"Beta_AF7\",  \"Beta_AF8\",  \"Beta_TP10\",\n",
    "  \"Gamma_TP9\", \"Gamma_AF7\", \"Gamma_AF8\", \"Gamma_TP10\"\n",
    "]\n",
    "\n",
    "# Find all subject folders sub-*\n",
    "subject_dirs = sorted(glob.glob(os.path.join(root_dir, \"sub-*\")))\n",
    "\n",
    "for subj_dir in subject_dirs:\n",
    "    subj_id = os.path.basename(subj_dir)\n",
    "    # locate the EEG CSV and log file by pattern\n",
    "    eeg_files = glob.glob(os.path.join(subj_dir, \"*mindMonitor*.csv\"))\n",
    "    log_files = glob.glob(os.path.join(subj_dir, \"*eeg_experiment_timeline_log*.txt\"))\n",
    "    if not eeg_files or not log_files:\n",
    "        print(f\"{subj_id}: missing data or log file, skipping\")\n",
    "        continue\n",
    "    eeg_file = eeg_files[0]\n",
    "    log_file = log_files[0]\n",
    "\n",
    "    # === 1) Load & clean EEG data ===\n",
    "    df = pd.read_csv(eeg_file)\n",
    "    if \"Elements\" in df.columns:\n",
    "        df = df[df[\"Elements\"].isna()]\n",
    "    df[\"Time\"] = pd.to_datetime(df[\"TimeStamp\"])\n",
    "    df.set_index(\"Time\", inplace=True)\n",
    "\n",
    "    # === 2) Load sync log & build intervals ===\n",
    "    sync_df = pd.read_csv(log_file, sep=\"\\t\", header=None, names=[\"Event\",\"Time\"])\n",
    "    sync_df.dropna(inplace=True)\n",
    "    exp0 = float(sync_df.loc[sync_df[\"Event\"].str.contains(\"Experiment Unix Start\"), \"Time\"].iat[0])\n",
    "    t0   = exp0 + 2*3600\n",
    "    to_ts = lambda offs: pd.to_datetime(t0 + float(offs), unit=\"s\")\n",
    "\n",
    "    intervals = {\"Baseline\": [], \"Task\": [], \"Rest\": [], \"Music\": []}\n",
    "    # Baseline\n",
    "    b0 = sync_df.loc[sync_df[\"Event\"]==\"Baseline Start\", \"Time\"].iat[0]\n",
    "    b1 = sync_df.loc[sync_df[\"Event\"]==\"Baseline End\",   \"Time\"].iat[0]\n",
    "    intervals[\"Baseline\"].append((to_ts(b0), to_ts(b1)))\n",
    "    # Task & Rest\n",
    "    for i,row in sync_df.iterrows():\n",
    "        ev, offs = row[\"Event\"], row[\"Time\"]\n",
    "        if \"Task Start\" in ev:\n",
    "            intervals[\"Task\"].append((to_ts(offs), to_ts(sync_df.at[i+1,\"Time\"])))\n",
    "        if \"Rest Start\" in ev:\n",
    "            intervals[\"Rest\"].append((to_ts(offs), to_ts(sync_df.at[i+1,\"Time\"])))\n",
    "    # Music\n",
    "    if \"Music Start\" in sync_df[\"Event\"].values:\n",
    "        m0 = sync_df.loc[sync_df[\"Event\"]==\"Music Start\", \"Time\"].iat[0]\n",
    "        m1 = sync_df.loc[sync_df[\"Event\"]==\"Music End\",   \"Time\"].iat[0]\n",
    "        intervals[\"Music\"].append((to_ts(m0), to_ts(m1)))\n",
    "\n",
    "    # === 3) Label each sample ===\n",
    "    df[\"Condition\"] = \"Unlabeled\"\n",
    "    for cond, spans in intervals.items():\n",
    "        for start, end in spans:\n",
    "            df.loc[(df.index>=start)&(df.index<=end), \"Condition\"] = cond\n",
    "    df = df[df[\"Condition\"]!=\"Unlabeled\"]\n",
    "\n",
    "    # === 4) Select features & build arrays ===\n",
    "    if selected_features:\n",
    "        missing = set(selected_features) - set(df.columns)\n",
    "        if missing:\n",
    "            print(f\"{subj_id}: warning, missing selected_features {missing}\")\n",
    "        feature_cols = [c for c in selected_features if c in df.columns]\n",
    "    else:\n",
    "        feature_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "    X      = df[feature_cols].T.values      # (n_features, n_timesteps)\n",
    "    y_true = df[\"Condition\"].values         # (n_timesteps,)\n",
    "    times  = df.index.to_numpy()            # (n_timesteps,)\n",
    "\n",
    "    # === 5) Encode states & estimate HMM params ===\n",
    "    states    = sorted(np.unique(y_true))\n",
    "    state2idx = {s:i for i,s in enumerate(states)}\n",
    "    idx2state = {i:s for s,i in state2idx.items()}\n",
    "    y_idx     = np.array([state2idx[s] for s in y_true])\n",
    "    n_states  = len(states)\n",
    "\n",
    "    # π\n",
    "    pi = np.zeros(n_states); pi[y_idx[0]] = 1.0\n",
    "    # A\n",
    "    A_counts = np.zeros((n_states,n_states))\n",
    "    for a,b in zip(y_idx[:-1], y_idx[1:]):\n",
    "        A_counts[a,b] += 1\n",
    "    A = (A_counts.T / A_counts.sum(axis=1)).T\n",
    "    for i in range(n_states):\n",
    "        if A[i].sum()==0: A[i,i]=1.0\n",
    "\n",
    "    # emissions\n",
    "    means, covars = [], []\n",
    "    reg = 1e-2\n",
    "    for i in range(n_states):\n",
    "        obs = X[:, y_idx==i].T\n",
    "        mu  = obs.mean(axis=0)\n",
    "        C   = np.cov(obs, rowvar=False)\n",
    "        C   = 0.5*(C + C.T) + reg*np.eye(C.shape[1])\n",
    "        means.append(mu); covars.append(C)\n",
    "    means  = np.vstack(means)\n",
    "    covars = np.stack(covars)\n",
    "\n",
    "    # === 6) Build HMM & decode ===\n",
    "    model = hmm.GaussianHMM(n_components=n_states,\n",
    "                            covariance_type=\"full\",\n",
    "                            init_params=\"\")\n",
    "    model.startprob_ = pi\n",
    "    model.transmat_  = A\n",
    "    model.means_     = means\n",
    "    model.covars_    = covars\n",
    "\n",
    "    _, y_pred_idx = model.decode(X.T, algorithm=\"viterbi\")\n",
    "    y_pred = np.array([idx2state[i] for i in y_pred_idx])\n",
    "\n",
    "    # accuracy\n",
    "    acc = (y_pred == y_true).mean()\n",
    "    print(f\"{subj_id}: Viterbi accuracy = {acc*100:.2f}%\")\n",
    "\n",
    "    # === 7) Plot ===\n",
    "    state_y  = {s:i for i,s in enumerate(states)}\n",
    "    y_true_i = [state_y[s] for s in y_true]\n",
    "    y_pred_i = [state_y[s] for s in y_pred]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10,3))\n",
    "    ax.step(times, y_true_i, where=\"post\", linestyle=\"--\", label=\"True\",   lw=1)\n",
    "    ax.step(times, y_pred_i, where=\"post\", linestyle=\"-\",  label=\"Predicted\", lw=1.5)\n",
    "    ax.set_yticks(list(state_y.values())); ax.set_yticklabels(states)\n",
    "    ax.set_title(f\"{subj_id}: True vs. Predicted (acc {acc*100:.1f}%)\")\n",
    "    ax.set_xlabel(\"Time\"); ax.grid(axis=\"x\", linestyle=\"--\", alpha=0.3)\n",
    "    ax.legend(loc=\"upper right\", fontsize=\"small\")\n",
    "    fig.autofmt_xdate()\n",
    "    plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from hmmlearn import hmm\n",
    "\n",
    "# === User settings ===\n",
    "root_dir = \"/Users/kyrusmama/Downloads/ref_prep_synchr_MAT_jiahui/Data_MAT\"  # directory containing sub-1, sub-2, etc.\n",
    "selected_features = [\n",
    "  \"Delta_TP9\", \"Delta_AF7\", \"Delta_AF8\", \"Delta_TP10\",\n",
    "  \"Theta_TP9\", \"Theta_AF7\", \"Theta_AF8\", \"Theta_TP10\",\n",
    "  \"Alpha_TP9\", \"Alpha_AF7\", \"Alpha_AF8\", \"Alpha_TP10\",\n",
    "  \"Beta_TP9\",  \"Beta_AF7\",  \"Beta_AF8\",  \"Beta_TP10\",\n",
    "  \"Gamma_TP9\", \"Gamma_AF7\", \"Gamma_AF8\", \"Gamma_TP10\"\n",
    "]\n",
    "\n",
    "# Find all subject folders sub-*\n",
    "subject_dirs = sorted(glob.glob(os.path.join(root_dir, \"sub-*\")))\n",
    "\n",
    "for subj_dir in subject_dirs:\n",
    "    subj_id = os.path.basename(subj_dir)\n",
    "    eeg_files = glob.glob(os.path.join(subj_dir, \"*mindMonitor*.csv\"))\n",
    "    log_files = glob.glob(os.path.join(subj_dir, \"*eeg_experiment_timeline_log*.txt\"))\n",
    "    if not eeg_files or not log_files:\n",
    "        print(f\"{subj_id}: missing data or log file, skipping\")\n",
    "        continue\n",
    "    eeg_file, log_file = eeg_files[0], log_files[0]\n",
    "\n",
    "    # === 1) Load & clean EEG data ===\n",
    "    df = pd.read_csv(eeg_file)\n",
    "    if \"Elements\" in df.columns:\n",
    "        df = df[df[\"Elements\"].isna()]\n",
    "    df[\"Time\"] = pd.to_datetime(df[\"TimeStamp\"])\n",
    "    df.set_index(\"Time\", inplace=True)\n",
    "\n",
    "    # === 2) Parse sync log & build intervals ===\n",
    "    sync_df = pd.read_csv(log_file, sep=\"\\t\", header=None, names=[\"Event\",\"Time\"])\n",
    "    sync_df.dropna(inplace=True)\n",
    "    exp0 = float(sync_df.loc[sync_df[\"Event\"].str.contains(\"Experiment Unix Start\"), \"Time\"].iat[0])\n",
    "    t0   = exp0 + 2*3600\n",
    "    to_ts = lambda offs: pd.to_datetime(t0 + float(offs), unit=\"s\")\n",
    "\n",
    "    intervals = {\"Baseline\": [], \"Task\": [], \"Rest\": [], \"Music\": []}\n",
    "    # Baseline\n",
    "    b0 = sync_df.loc[sync_df[\"Event\"]==\"Baseline Start\", \"Time\"].iat[0]\n",
    "    b1 = sync_df.loc[sync_df[\"Event\"]==\"Baseline End\",   \"Time\"].iat[0]\n",
    "    intervals[\"Baseline\"].append((to_ts(b0), to_ts(b1)))\n",
    "    # Task & Rest\n",
    "    for i,row in sync_df.iterrows():\n",
    "        ev, offs = row[\"Event\"], row[\"Time\"]\n",
    "        if \"Task Start\" in ev:\n",
    "            intervals[\"Task\"].append((to_ts(offs), to_ts(sync_df.at[i+1,\"Time\"])))\n",
    "        if \"Rest Start\" in ev:\n",
    "            intervals[\"Rest\"].append((to_ts(offs), to_ts(sync_df.at[i+1,\"Time\"])))\n",
    "    # Music\n",
    "    if \"Music Start\" in sync_df[\"Event\"].values:\n",
    "        m0 = sync_df.loc[sync_df[\"Event\"]==\"Music Start\", \"Time\"].iat[0]\n",
    "        m1 = sync_df.loc[sync_df[\"Event\"]==\"Music End\",   \"Time\"].iat[0]\n",
    "        intervals[\"Music\"].append((to_ts(m0), to_ts(m1)))\n",
    "\n",
    "    # === 3) Label each sample ===\n",
    "    df[\"Condition\"] = \"Unlabeled\"\n",
    "    for cond, spans in intervals.items():\n",
    "        for start, end in spans:\n",
    "            df.loc[(df.index>=start)&(df.index<=end), \"Condition\"] = cond\n",
    "    df = df[df[\"Condition\"]!=\"Unlabeled\"]\n",
    "\n",
    "    # === 4) Plot raw channel signals ===\n",
    "    channels = [\"TP9\",\"AF7\",\"AF8\",\"TP10\"]\n",
    "    raw_cols = [f\"RAW_{ch}\" for ch in channels if f\"RAW_{ch}\" in df.columns]\n",
    "    if raw_cols:\n",
    "        fig, axs = plt.subplots(len(raw_cols), 1, figsize=(12, 2.5*len(raw_cols)), sharex=True)\n",
    "        if len(raw_cols)==1: axs=[axs]\n",
    "        for ax, col in zip(axs, raw_cols):\n",
    "            ax.plot(df.index, df[col], linewidth=0.8)\n",
    "            ax.set_ylabel(col)\n",
    "            ax.grid(alpha=0.3)\n",
    "        axs[-1].set_xlabel(\"Time\")\n",
    "        fig.suptitle(f\"{subj_id} – Raw EEG Channels\", y=1.02)\n",
    "        plt.tight_layout()\n",
    "\n",
    "    # === 5) Plot band-power by channel ===\n",
    "    bands = [\"Delta\",\"Theta\",\"Alpha\",\"Beta\",\"Gamma\"]\n",
    "    fig, axs = plt.subplots(len(bands), 1, figsize=(12, 2.5*len(bands)), sharex=True)\n",
    "    if len(bands)==1: axs=[axs]\n",
    "    for ax, band in zip(axs, bands):\n",
    "        for ch in channels:\n",
    "            col = f\"{band}_{ch}\"\n",
    "            if col in df.columns:\n",
    "                ax.plot(df.index, df[col].abs(), label=ch, linewidth=1)\n",
    "        ax.set_ylabel(f\"{band} power\")\n",
    "        ax.legend(loc=\"upper right\", fontsize=\"small\")\n",
    "        ax.grid(alpha=0.3)\n",
    "    axs[-1].set_xlabel(\"Time\")\n",
    "    fig.suptitle(f\"{subj_id} – Band Power by Channel\", y=1.02)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # === 6) Prepare data for HMM ===\n",
    "    feature_cols = [c for c in selected_features if c in df.columns]\n",
    "    if not feature_cols:\n",
    "        feature_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    X      = df[feature_cols].T.values      # (n_features, n_timesteps)\n",
    "    y_true = df[\"Condition\"].values         # (n_timesteps,)\n",
    "    times  = df.index.to_numpy()            # (n_timesteps,)\n",
    "\n",
    "    # === 7) Encode states & estimate HMM params ===\n",
    "    states    = sorted(np.unique(y_true))\n",
    "    state2idx = {s:i for i,s in enumerate(states)}\n",
    "    idx2state = {i:s for s,i in state2idx.items()}\n",
    "    y_idx     = np.array([state2idx[s] for s in y_true])\n",
    "    n_states  = len(states)\n",
    "\n",
    "    # π\n",
    "    pi = np.zeros(n_states); pi[y_idx[0]] = 1.0\n",
    "    # A\n",
    "    A_counts = np.zeros((n_states,n_states))\n",
    "    for a,b in zip(y_idx[:-1], y_idx[1:]):\n",
    "        A_counts[a,b] += 1\n",
    "    A = (A_counts.T / A_counts.sum(axis=1)).T\n",
    "    for i in range(n_states):\n",
    "        if A[i].sum()==0: A[i,i]=1.0\n",
    "\n",
    "    # emissions\n",
    "    means, covars = [], []\n",
    "    reg = 1e-2\n",
    "    for i in range(n_states):\n",
    "        obs = X[:, y_idx==i].T\n",
    "        mu  = obs.mean(axis=0)\n",
    "        C   = np.cov(obs, rowvar=False)\n",
    "        C   = 0.5*(C + C.T) + reg*np.eye(C.shape[1])\n",
    "        means.append(mu); covars.append(C)\n",
    "    means  = np.vstack(means)\n",
    "    covars = np.stack(covars)\n",
    "\n",
    "    # === 8) Build HMM & decode ===\n",
    "    model = hmm.GaussianHMM(n_components=n_states,\n",
    "                            covariance_type=\"full\",\n",
    "                            init_params=\"\")\n",
    "    model.startprob_, model.transmat_, model.means_, model.covars_ = pi, A, means, covars\n",
    "\n",
    "    _, y_pred_idx = model.decode(X.T, algorithm=\"viterbi\")\n",
    "    y_pred = np.array([idx2state[i] for i in y_pred_idx])\n",
    "\n",
    "    # accuracy\n",
    "    acc = (y_pred == y_true).mean()\n",
    "    print(f\"{subj_id}: Viterbi accuracy = {acc*100:.2f}%\")\n",
    "\n",
    "    # === 9) Plot true vs predicted ===\n",
    "    state_y  = {s:i for i,s in enumerate(states)}\n",
    "    y_true_i = [state_y[s] for s in y_true]\n",
    "    y_pred_i = [state_y[s] for s in y_pred]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10,3))\n",
    "    ax.step(times, y_true_i, where=\"post\", linestyle=\"--\", label=\"True\",   lw=1)\n",
    "    ax.step(times, y_pred_i, where=\"post\", linestyle=\"-\",  label=\"Predicted\", lw=1.5)\n",
    "    ax.set_yticks(list(state_y.values())); ax.set_yticklabels(states)\n",
    "    ax.set_title(f\"{subj_id}: True vs. Predicted (acc {acc*100:.1f}%)\")\n",
    "    ax.set_xlabel(\"Time\"); ax.grid(axis=\"x\", linestyle=\"--\", alpha=0.3)\n",
    "    ax.legend(loc=\"upper right\", fontsize=\"small\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import butter, sosfiltfilt, hilbert\n",
    "\n",
    "# === User settings ===\n",
    "root_dir = \"/Users/kyrusmama/Downloads/ref_prep_synchr_MAT_jiahui/Data_MAT\"  # parent folder containing sub-1, sub-2, etc.\n",
    "\n",
    "# define the four frequency bands\n",
    "bands = {\n",
    "    \"Delta\": (0.5, 4),\n",
    "    \"Theta\": (4,   8),\n",
    "    \"Alpha\": (8,  13),\n",
    "    \"Beta\":  (13, 30),\n",
    "}\n",
    "\n",
    "# find all subject subdirectories\n",
    "subject_dirs = sorted(glob.glob(os.path.join(root_dir, \"sub-*\")))\n",
    "\n",
    "for subj_dir in subject_dirs:\n",
    "    subj_id = os.path.basename(subj_dir)\n",
    "    # locate the mindMonitor CSV\n",
    "    eeg_files = glob.glob(os.path.join(subj_dir, \"*mindMonitor*.csv\"))\n",
    "    if not eeg_files:\n",
    "        print(f\"{subj_id}: no mindMonitor file found, skipping\")\n",
    "        continue\n",
    "    eeg_file = eeg_files[0]\n",
    "\n",
    "    # === 1) Load & clean raw EEG ===\n",
    "    df = pd.read_csv(eeg_file)\n",
    "    # drop any Muse \"elements\" event rows\n",
    "    if \"Elements\" in df.columns:\n",
    "        df = df[df[\"Elements\"].isna()]\n",
    "    df[\"Time\"] = pd.to_datetime(df[\"TimeStamp\"])\n",
    "    df.set_index(\"Time\", inplace=True)\n",
    "\n",
    "    # pick the four raw channels\n",
    "    raw_cols = [c for c in [\"RAW_TP9\",\"RAW_AF7\",\"RAW_AF8\",\"RAW_TP10\"] if c in df.columns]\n",
    "    if not raw_cols:\n",
    "        print(f\"{subj_id}: no RAW_* channels found, skipping\")\n",
    "        continue\n",
    "    raw = df[raw_cols]\n",
    "\n",
    "    # estimate sampling rate\n",
    "    # dt  = raw.index.to_series().diff().dt.total_seconds().median()\n",
    "    fs  = 2.0 \n",
    "\n",
    "    # === 2) Compute band‐power for each band ===\n",
    "    band_power = {}\n",
    "    for name, (low, high) in bands.items():\n",
    "        # design a 4th‐order Butterworth bandpass\n",
    "        sos = butter(4, [low, high], btype=\"bandpass\", fs=fs, output=\"sos\")\n",
    "        # zero‐phase filter\n",
    "        filt = sosfiltfilt(sos, raw.values, axis=0)\n",
    "        # analytic signal → envelope\n",
    "        env  = np.abs(hilbert(filt, axis=0))\n",
    "        # instantaneous power\n",
    "        power = env**2\n",
    "        # average across the four channels\n",
    "        band_power[name] = pd.Series(power.mean(axis=1), index=raw.index)\n",
    "\n",
    "    # === 3) Plot results for this subject ===\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for name, ts in band_power.items():\n",
    "        plt.plot(ts.index, ts, label=name)\n",
    "    plt.title(f\"Subject {subj_id} — Band Power Over Time\")\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Mean Power\")\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from hmmlearn import hmm\n",
    "\n",
    "# === User settings ===\n",
    "root_dir = \"/Users/kyrusmama/Downloads/ref_prep_synchr_MAT_jiahui/Data_MGR\"  # contains sub-1, sub-2, etc.\n",
    "# List specific EEG feature columns to use, or leave empty to use all numeric columns:\n",
    "selected_features = [\n",
    "  \"Delta_TP9\", \"Delta_AF7\", \"Delta_AF8\", \"Delta_TP10\",\n",
    "  \"Theta_TP9\", \"Theta_AF7\", \"Theta_AF8\", \"Theta_TP10\",\n",
    "  \"Alpha_TP9\", \"Alpha_AF7\", \"Alpha_AF8\", \"Alpha_TP10\",\n",
    "  \"Beta_TP9\",  \"Beta_AF7\",  \"Beta_AF8\",  \"Beta_TP10\",\n",
    "  \"Gamma_TP9\", \"Gamma_AF7\", \"Gamma_AF8\", \"Gamma_TP10\"\n",
    "]\n",
    "\n",
    "\n",
    "# find all subject folders\n",
    "subject_dirs = sorted(glob.glob(os.path.join(root_dir, \"sub-*\")))\n",
    "\n",
    "for subj_dir in subject_dirs:\n",
    "    subj_id = os.path.basename(subj_dir)\n",
    "    # locate mindMonitor CSV and log txt\n",
    "    eeg_files = glob.glob(os.path.join(subj_dir, \"*mindMonitor*.csv\"))\n",
    "    log_files = glob.glob(os.path.join(subj_dir, \"*music_genre_experiment_log*.txt\"))\n",
    "    if not eeg_files or not log_files:\n",
    "        print(f\"{subj_id}: missing data or log → skipping\")\n",
    "        continue\n",
    "    eeg_file = eeg_files[0]\n",
    "    log_file = log_files[0]\n",
    "\n",
    "    # --- 1) Load & clean EEG data ---\n",
    "    df = pd.read_csv(eeg_file)\n",
    "    # drop blink/jaw events\n",
    "    if \"Elements\" in df.columns:\n",
    "        df = df[df[\"Elements\"].isna()]\n",
    "    df[\"Time\"] = pd.to_datetime(df[\"TimeStamp\"])\n",
    "    df.set_index(\"Time\", inplace=True)\n",
    "\n",
    "    # --- 2) Load sync log & build intervals ---\n",
    "    sync_df = pd.read_csv(log_file, sep=\"\\t\", header=None, names=[\"Event\",\"Time\"])\n",
    "    sync_df.dropna(inplace=True)\n",
    "    # experiment unix start + 2h offset\n",
    "    exp0 = float(sync_df.loc[sync_df[\"Event\"].str.contains(\"Experiment Unix Start\"), \"Time\"].iat[0])\n",
    "    t0   = exp0 + 2*3600\n",
    "    to_ts = lambda offs: pd.to_datetime(t0 + float(offs), unit=\"s\")\n",
    "\n",
    "    # intervals for each state\n",
    "    intervals = {\n",
    "        \"Rest\": [],     # includes baseline + post‐rest\n",
    "        \"Jazz\": [], \n",
    "        \"Classical\": [], \n",
    "        \"Electronic\": []\n",
    "    }\n",
    "\n",
    "    # baseline → Rest\n",
    "    b0 = sync_df.loc[sync_df[\"Event\"]==\"Baseline Start\", \"Time\"].iat[0]\n",
    "    b1 = sync_df.loc[sync_df[\"Event\"]==\"Baseline End\",   \"Time\"].iat[0]\n",
    "    intervals[\"Rest\"].append((to_ts(b0), to_ts(b1)))\n",
    "\n",
    "    # music trials and post‐rest\n",
    "    for i, row in sync_df.iterrows():\n",
    "        ev, offs = row[\"Event\"], row[\"Time\"]\n",
    "        if ev.startswith(\"Music Start\"):\n",
    "            # format: \"Music Start - Trial X - Genre - Duration ...\"\n",
    "            parts = ev.split(\" - \")\n",
    "            genre = parts[2]\n",
    "            start = to_ts(offs)\n",
    "            # next row is Music End\n",
    "            end   = to_ts(sync_df.at[i+1, \"Time\"])\n",
    "            intervals[genre].append((start, end))\n",
    "        elif ev.startswith(\"Post Rest Start\"):\n",
    "            start = to_ts(offs)\n",
    "            end   = to_ts(sync_df.at[i+1, \"Time\"])\n",
    "            intervals[\"Rest\"].append((start, end))\n",
    "\n",
    "    # --- 3) Label each EEG sample ---\n",
    "    df[\"Condition\"] = \"Unlabeled\"\n",
    "    for cond, spans in intervals.items():\n",
    "        for start, end in spans:\n",
    "            mask = (df.index >= start) & (df.index <= end)\n",
    "            df.loc[mask, \"Condition\"] = cond\n",
    "    df = df[df[\"Condition\"] != \"Unlabeled\"]\n",
    "\n",
    "    # --- 4) Select features & build arrays ---\n",
    "    if selected_features:\n",
    "        missing = set(selected_features) - set(df.columns)\n",
    "        if missing:\n",
    "            print(f\"{subj_id}: warning, missing selected_features {missing}\")\n",
    "        feature_cols = [c for c in selected_features if c in df.columns]\n",
    "    else:\n",
    "        feature_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "    X      = df[feature_cols].T.values      # (n_features, n_timesteps)\n",
    "    y_true = df[\"Condition\"].values         # (n_timesteps,)\n",
    "    times  = df.index.to_numpy()            # (n_timesteps,)\n",
    "\n",
    "    # --- 5) Encode states & estimate HMM params ---\n",
    "    states    = sorted(np.unique(y_true))\n",
    "    state2idx = {s:i for i,s in enumerate(states)}\n",
    "    idx2state = {i:s for s,i in state2idx.items()}\n",
    "    # print(state2idx)\n",
    "    # print(y_true)\n",
    "    y_idx     = np.array([state2idx[s] for s in y_true])\n",
    "    # print(y_idx)\n",
    "\n",
    "    n_states  = len(states)\n",
    "\n",
    "    # initial π\n",
    "    pi = np.zeros(n_states); pi[y_idx[0]] = 1.0\n",
    "\n",
    "    # transition matrix A\n",
    "    A_counts = np.zeros((n_states, n_states))\n",
    "    for a,b in zip(y_idx[:-1], y_idx[1:]):\n",
    "        A_counts[a,b] += 1\n",
    "    A = (A_counts.T / A_counts.sum(axis=1)).T\n",
    "    for i in range(n_states):\n",
    "        if A[i].sum() == 0:\n",
    "            A[i,i] = 1.0\n",
    "\n",
    "    # emission Gaussians\n",
    "    means, covars = [], []\n",
    "    reg = 1e-2\n",
    "    for i in range(n_states):\n",
    "        obs = X[:, y_idx==i].T\n",
    "        mu  = obs.mean(axis=0)\n",
    "        C   = np.cov(obs, rowvar=False)\n",
    "        C   = 0.5*(C + C.T) + reg*np.eye(C.shape[1])\n",
    "        means.append(mu)\n",
    "        covars.append(C)\n",
    "    means  = np.vstack(means)\n",
    "    covars = np.stack(covars)\n",
    "\n",
    "    # --- 6) Build HMM & decode via Viterbi ---\n",
    "    model = hmm.GaussianHMM(n_components=n_states,\n",
    "                            covariance_type=\"full\",\n",
    "                            init_params=\"\")\n",
    "    model.startprob_ = pi\n",
    "    model.transmat_  = A\n",
    "    model.means_     = means\n",
    "    model.covars_    = covars\n",
    "\n",
    "    _, y_pred_idx = model.decode(X.T, algorithm=\"viterbi\")\n",
    "    y_pred = np.array([idx2state[i] for i in y_pred_idx])\n",
    "\n",
    "    # accuracy\n",
    "    acc = (y_pred == y_true).mean()\n",
    "    print(f\"{subj_id}: Viterbi accuracy = {acc*100:.2f}%\")\n",
    "\n",
    "    # --- 7) Plot true vs predicted states ---\n",
    "    state_y    = {s:i for i,s in enumerate(states)}\n",
    "    y_true_i   = [state_y[s] for s in y_true]\n",
    "    y_pred_i   = [state_y[s] for s in y_pred]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10,3))\n",
    "    ax.step(times, y_true_i, where=\"post\", linestyle=\"--\", label=\"True\",   lw=1)\n",
    "    ax.step(times, y_pred_i, where=\"post\", linestyle=\"-\",  label=\"Predicted\", lw=1.5)\n",
    "    ax.set_yticks(list(state_y.values())); ax.set_yticklabels(states)\n",
    "    ax.set_title(f\"{subj_id}: True vs Predicted (acc {acc*100:.1f}%)\")\n",
    "    ax.set_xlabel(\"Time\"); ax.grid(axis=\"x\", linestyle=\"--\", alpha=0.3)\n",
    "    ax.legend(loc=\"upper right\", fontsize=\"small\")\n",
    "    fig.autofmt_xdate()\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # === 8) Plot example emission PDFs for the first selected feature ===\n",
    "    if feature_cols:\n",
    "        feat_idx  = 1\n",
    "        feat_name = feature_cols[feat_idx]\n",
    "        vals      = X[feat_idx, :]\n",
    "        xgrid     = np.linspace(vals.min(), vals.max(), 200)\n",
    "\n",
    "        fig2, ax2 = plt.subplots(figsize=(8,4))\n",
    "        for i,state in enumerate(states):\n",
    "            mu  = means[i, feat_idx]\n",
    "            var = covars[i, feat_idx, feat_idx]\n",
    "            pdf = 1/np.sqrt(2*np.pi*var) * np.exp(-0.5*((xgrid-mu)**2/var))\n",
    "            ax2.plot(xgrid, pdf, label=state)\n",
    "        ax2.set_xlabel(feat_name)\n",
    "        ax2.set_ylabel(\"Density\")\n",
    "        ax2.set_title(f\"Emission PDFs for “{feat_name}”\")\n",
    "        ax2.legend()\n",
    "        plt.tight_layout()\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from hmmlearn import hmm\n",
    "\n",
    "# === User settings ===\n",
    "root_dir = \"/Users/kyrusmama/Downloads/ref_prep_synchr_MAT_jiahui/Data_MGR\"\n",
    "selected_features = [\n",
    "  \"Delta_TP9\", \"Delta_AF7\", \"Delta_AF8\", \"Delta_TP10\",\n",
    "  \"Theta_TP9\", \"Theta_AF7\", \"Theta_AF8\", \"Theta_TP10\",\n",
    "  \"Alpha_TP9\", \"Alpha_AF7\", \"Alpha_AF8\", \"Alpha_TP10\",\n",
    "  \"Beta_TP9\",  \"Beta_AF7\",  \"Beta_AF8\",  \"Beta_TP10\",\n",
    "  \"Gamma_TP9\", \"Gamma_AF7\", \"Gamma_AF8\", \"Gamma_TP10\"\n",
    "]\n",
    "\n",
    "subject_dirs = sorted(glob.glob(os.path.join(root_dir, \"sub-*\")))\n",
    "\n",
    "for subj_dir in subject_dirs:\n",
    "    subj_id = os.path.basename(subj_dir)\n",
    "    # find files\n",
    "    eeg_files = glob.glob(os.path.join(subj_dir, \"*mindMonitor*.csv\"))\n",
    "    log_files = glob.glob(os.path.join(subj_dir, \"*music_genre_experiment_log*.txt\"))\n",
    "    if not eeg_files or not log_files:\n",
    "        print(f\"{subj_id}: missing data/log → skipping\")\n",
    "        continue\n",
    "    eeg_file, log_file = eeg_files[0], log_files[0]\n",
    "\n",
    "    # --- 1) Load & clean EEG ---\n",
    "    df = pd.read_csv(eeg_file)\n",
    "    if \"Elements\" in df.columns:\n",
    "        df = df[df[\"Elements\"].isna()]\n",
    "    df[\"Time\"] = pd.to_datetime(df[\"TimeStamp\"])\n",
    "    df.set_index(\"Time\", inplace=True)\n",
    "\n",
    "    # --- 2) Parse log & build intervals ---\n",
    "    sync_df = pd.read_csv(log_file, sep=\"\\t\", header=None, names=[\"Event\",\"Time\"])\n",
    "    sync_df.dropna(inplace=True)\n",
    "    exp0 = float(sync_df.loc[sync_df[\"Event\"].str.contains(\"Experiment Unix Start\"), \"Time\"].iat[0])\n",
    "    t0   = exp0 + 2*3600\n",
    "    to_ts = lambda offs: pd.to_datetime(t0 + float(offs), unit=\"s\")\n",
    "\n",
    "    intervals = {\"Rest\": [], \"Jazz\": [], \"Classical\": [], \"Electronic\": []}\n",
    "    # Baseline → Rest\n",
    "    b0 = sync_df.loc[sync_df[\"Event\"]==\"Baseline Start\", \"Time\"].iat[0]\n",
    "    b1 = sync_df.loc[sync_df[\"Event\"]==\"Baseline End\",   \"Time\"].iat[0]\n",
    "    intervals[\"Rest\"].append((to_ts(b0), to_ts(b1)))\n",
    "\n",
    "    # Music & Post‐Rest\n",
    "    for i,row in sync_df.iterrows():\n",
    "        ev, offs = row[\"Event\"], row[\"Time\"]\n",
    "        if ev.startswith(\"Music Start\"):\n",
    "            genre = ev.split(\" - \")[2]\n",
    "            start, end = to_ts(offs), to_ts(sync_df.at[i+1,\"Time\"])\n",
    "            intervals[genre].append((start,end))\n",
    "        elif ev.startswith(\"Post Rest Start\"):\n",
    "            start, end = to_ts(offs), to_ts(sync_df.at[i+1,\"Time\"])\n",
    "            intervals[\"Rest\"].append((start,end))\n",
    "\n",
    "    # --- 3) Label each sample ---\n",
    "    df[\"Condition\"] = \"Unlabeled\"\n",
    "    for cond, spans in intervals.items():\n",
    "        for start,end in spans:\n",
    "            df.loc[(df.index>=start)&(df.index<=end),\"Condition\"] = cond\n",
    "    df = df[df[\"Condition\"]!=\"Unlabeled\"]\n",
    "\n",
    "    # --- 4) Select features & build arrays ---\n",
    "    feature_cols = [c for c in selected_features if c in df.columns]\n",
    "    X      = df[feature_cols].T.values      # (n_features, n_timesteps)\n",
    "    y_true = df[\"Condition\"].values         # (n_timesteps,)\n",
    "    times  = df.index.to_numpy()            # (n_timesteps,)\n",
    "\n",
    "    # --- 5) Encode states & estimate HMM params ---\n",
    "    states    = sorted(np.unique(y_true))\n",
    "    state2idx = {s:i for i,s in enumerate(states)}\n",
    "    idx2state = {i:s for s,i in state2idx.items()}\n",
    "    y_idx     = np.array([state2idx[s] for s in y_true])\n",
    "    n_states  = len(states)\n",
    "\n",
    "    # π\n",
    "    pi = np.zeros(n_states); pi[y_idx[0]] = 1.0\n",
    "    # A\n",
    "    A_counts = np.zeros((n_states,n_states))\n",
    "    for a,b in zip(y_idx[:-1], y_idx[1:]):\n",
    "        A_counts[a,b] += 1\n",
    "    A = (A_counts.T / A_counts.sum(axis=1)).T\n",
    "    for i in range(n_states):\n",
    "        if A[i].sum()==0: A[i,i]=1.0\n",
    "\n",
    "    # Emission Gaussians\n",
    "    means, covars = [], []\n",
    "    reg = 1e-2\n",
    "    for i in range(n_states):\n",
    "        obs = X[:, y_idx==i].T\n",
    "        mu  = obs.mean(axis=0)\n",
    "        C   = np.cov(obs, rowvar=False)\n",
    "        C   = 0.5*(C + C.T) + reg*np.eye(C.shape[1])\n",
    "        means.append(mu)\n",
    "        covars.append(C)\n",
    "    means  = np.vstack(means)              # (n_states, n_feats)\n",
    "    covars = np.stack(covars)              # (n_states, n_feats, n_feats)\n",
    "\n",
    "    # --- 5a) Compute Fisher‐ratio per feature ---\n",
    "    counts = np.array([np.sum(y_idx==i) for i in range(n_states)])\n",
    "    fisher_scores = []\n",
    "    for j in range(len(feature_cols)):\n",
    "        # global mean of feature j\n",
    "        global_mean = np.dot(counts, means[:,j]) / counts.sum()\n",
    "        between = np.sum(counts * (means[:,j] - global_mean)**2)\n",
    "        within  = np.sum(counts * covars[:,j,j])\n",
    "        score   = between/within if within>0 else 0\n",
    "        fisher_scores.append(score)\n",
    "\n",
    "    best_j      = int(np.argmax(fisher_scores))\n",
    "    best_feat   = feature_cols[best_j]\n",
    "    print(f\"{subj_id}: most discriminative feature → {best_feat}\")\n",
    "\n",
    "    # --- 6) Plot its emission PDFs ---\n",
    "    xvals = X[best_j,:]\n",
    "    grid  = np.linspace(xvals.min(), xvals.max(), 200)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8,4))\n",
    "    for i, st in enumerate(states):\n",
    "        mu  = means[i, best_j]\n",
    "        var = covars[i, best_j, best_j]\n",
    "        pdf = 1/np.sqrt(2*np.pi*var) * np.exp(-0.5*(grid-mu)**2/var)\n",
    "        ax.plot(grid, pdf, label=st)\n",
    "    ax.set_xlabel(best_feat)\n",
    "    ax.set_ylabel(\"Density\")\n",
    "    ax.set_title(f\"{subj_id}: Emission PDFs for “{best_feat}”\")\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # --- 7) (Optional) run & plot Viterbi as before ---\n",
    "    model = hmm.GaussianHMM(n_components=n_states, covariance_type=\"full\", init_params=\"\")\n",
    "    model.startprob_, model.transmat_, model.means_, model.covars_ = pi, A, means, covars\n",
    "    _, y_pred_idx = model.decode(X.T, algorithm=\"viterbi\")\n",
    "    acc = (y_pred_idx == y_idx).mean()\n",
    "    print(f\"  Viterbi accuracy = {acc*100:.2f}%\")\n",
    "\n",
    "    fig2, ax2 = plt.subplots(figsize=(10,3))\n",
    "    yt = [state2idx[s] for s in y_true]\n",
    "    yp = y_pred_idx\n",
    "    ax2.step(times, yt, where=\"post\", linestyle=\"--\", label=\"True\",   lw=1)\n",
    "    ax2.step(times, yp, where=\"post\", linestyle=\"-\",  label=\"Predicted\", lw=1.5)\n",
    "    ax2.set_yticks(range(n_states)); ax2.set_yticklabels(states)\n",
    "    ax2.set_title(f\"{subj_id}: True vs Pred (acc {acc*100:.1f}%)\")\n",
    "    ax2.set_xlabel(\"Time\"); ax2.grid(axis=\"x\", linestyle=\"--\", alpha=0.3)\n",
    "    ax2.legend(loc=\"upper right\", fontsize=\"small\")\n",
    "    fig2.autofmt_xdate()\n",
    "    plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from hmmlearn import hmm\n",
    "\n",
    "# === User settings ===\n",
    "root_dir = \"/Users/kyrusmama/Downloads/ref_prep_synchr_MAT_jiahui/Data_MGR\"  # contains sub-1, sub-2, etc.\n",
    "# List specific EEG feature columns to use, or leave empty to use all numeric columns:\n",
    "selected_features = [\n",
    "  \"Delta_TP9\", \"Delta_AF7\", \"Delta_AF8\", \"Delta_TP10\",\n",
    "  \"Theta_TP9\", \"Theta_AF7\", \"Theta_AF8\", \"Theta_TP10\",\n",
    "  \"Alpha_TP9\", \"Alpha_AF7\", \"Alpha_AF8\", \"Alpha_TP10\",\n",
    "  \"Beta_TP9\",  \"Beta_AF7\",  \"Beta_AF8\",  \"Beta_TP10\",\n",
    "#   \"Gamma_TP9\", \"Gamma_AF7\", \"Gamma_AF8\", \"Gamma_TP10\"\n",
    "]\n",
    "\n",
    "# find all subject folders\n",
    "subject_dirs = sorted(glob.glob(os.path.join(root_dir, \"sub-*\")))\n",
    "\n",
    "for subj_dir in subject_dirs:\n",
    "    subj_id = os.path.basename(subj_dir)\n",
    "    # locate mindMonitor CSV and log txt\n",
    "    eeg_files = glob.glob(os.path.join(subj_dir, \"*mindMonitor*.csv\"))\n",
    "    log_files = glob.glob(os.path.join(subj_dir, \"*music_genre_experiment_log*.txt\"))\n",
    "    if not eeg_files or not log_files:\n",
    "        print(f\"{subj_id}: missing data or log → skipping\")\n",
    "        continue\n",
    "    eeg_file = eeg_files[0]\n",
    "    log_file = log_files[0]\n",
    "\n",
    "    # --- 1) Load & clean EEG data ---\n",
    "    df = pd.read_csv(eeg_file)\n",
    "    # drop blink/jaw events\n",
    "    if \"Elements\" in df.columns:\n",
    "        df = df[df[\"Elements\"].isna()]\n",
    "    # parse timestamps & index\n",
    "    df[\"Time\"] = pd.to_datetime(df[\"TimeStamp\"])\n",
    "    df.set_index(\"Time\", inplace=True)\n",
    "\n",
    "    # --- 2) Load sync log & build intervals ---\n",
    "    sync_df = pd.read_csv(log_file, sep=\"\\t\", header=None, names=[\"Event\",\"Time\"])\n",
    "    sync_df.dropna(inplace=True)\n",
    "    # experiment unix start + 2h offset\n",
    "    exp0 = float(sync_df.loc[sync_df[\"Event\"].str.contains(\"Experiment Unix Start\"), \"Time\"].iat[0])\n",
    "    t0   = exp0 + 2*3600\n",
    "    to_ts = lambda offs: pd.to_datetime(t0 + float(offs), unit=\"s\")\n",
    "\n",
    "    intervals = {\n",
    "        \"Rest\": [],       # baseline + post-rest\n",
    "        \"Jazz\": [],\n",
    "        \"Classical\": [],\n",
    "        \"Electronic\": []\n",
    "    }\n",
    "    # baseline → Rest\n",
    "    b0 = sync_df.loc[sync_df[\"Event\"]==\"Baseline Start\", \"Time\"].iat[0]\n",
    "    b1 = sync_df.loc[sync_df[\"Event\"]==\"Baseline End\",   \"Time\"].iat[0]\n",
    "    intervals[\"Rest\"].append((to_ts(b0), to_ts(b1)))\n",
    "    # music & post-rest\n",
    "    for i, row in sync_df.iterrows():\n",
    "        ev, offs = row[\"Event\"], row[\"Time\"]\n",
    "        if ev.startswith(\"Music Start\"):\n",
    "            genre = ev.split(\" - \")[2]\n",
    "            start = to_ts(offs)\n",
    "            end   = to_ts(sync_df.at[i+1, \"Time\"])\n",
    "            intervals[genre].append((start, end))\n",
    "        elif ev.startswith(\"Post Rest Start\"):\n",
    "            start = to_ts(offs)\n",
    "            end   = to_ts(sync_df.at[i+1, \"Time\"])\n",
    "            intervals[\"Rest\"].append((start, end))\n",
    "\n",
    "    # --- 3) Label each EEG sample ---\n",
    "    df[\"Condition\"] = \"Unlabeled\"\n",
    "    for cond, spans in intervals.items():\n",
    "        for start, end in spans:\n",
    "            df.loc[(df.index>=start)&(df.index<=end), \"Condition\"] = cond\n",
    "    df = df[df[\"Condition\"] != \"Unlabeled\"]\n",
    "\n",
    "    # --- 4) Select features & build arrays ---\n",
    "    if selected_features:\n",
    "        missing = set(selected_features) - set(df.columns)\n",
    "        if missing:\n",
    "            print(f\"{subj_id}: warning, missing selected_features {missing}\")\n",
    "        feature_cols = [c for c in selected_features if c in df.columns]\n",
    "    else:\n",
    "        feature_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "    X      = df[feature_cols].T.values      # (n_features, n_timesteps)\n",
    "    y_true = df[\"Condition\"].values         # (n_timesteps,)\n",
    "    times  = df.index.to_numpy()            # (n_timesteps,)\n",
    "\n",
    "    # --- 4b) Plot band-power channels for each band ---\n",
    "    bands    = [\"Delta\", \"Theta\", \"Alpha\", \"Beta\", \"Gamma\"]\n",
    "    channels = [\"TP9\", \"AF7\", \"AF8\", \"TP10\"]\n",
    "\n",
    "    fig, axs = plt.subplots(len(bands), 1,\n",
    "                            figsize=(12, 2.5*len(bands)),\n",
    "                            sharex=True)\n",
    "    if len(bands) == 1:\n",
    "        axs = [axs]\n",
    "\n",
    "    for ax, band in zip(axs, bands):\n",
    "        for ch in channels:\n",
    "            col = f\"{band}_{ch}\"\n",
    "            if col in df.columns:\n",
    "                ax.plot(times,\n",
    "                        df[col].abs(),\n",
    "                        label=ch,\n",
    "                        linewidth=1)\n",
    "        ax.set_ylabel(f\"{band} Power\")\n",
    "        ax.legend(loc=\"upper right\", fontsize=\"small\")\n",
    "        ax.grid(alpha=0.3)\n",
    "\n",
    "    axs[-1].set_xlabel(\"Time\")\n",
    "    fig.suptitle(f\"{subj_id} – Band Power by Channel\", y=1.02)\n",
    "    fig.autofmt_xdate()\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "    # --- 5) Encode states & estimate HMM params ---\n",
    "    states    = sorted(np.unique(y_true))\n",
    "    state2idx = {s:i for i,s in enumerate(states)}\n",
    "    idx2state = {i:s for s,i in state2idx.items()}\n",
    "    y_idx     = np.array([state2idx[s] for s in y_true])\n",
    "    n_states  = len(states)\n",
    "\n",
    "    # initial π\n",
    "    pi = np.zeros(n_states); pi[y_idx[0]] = 1.0\n",
    "    # transition matrix A\n",
    "    A_counts = np.zeros((n_states, n_states))\n",
    "    for a,b in zip(y_idx[:-1], y_idx[1:]):\n",
    "        A_counts[a,b] += 1\n",
    "    A = (A_counts.T / A_counts.sum(axis=1)).T\n",
    "    for i in range(n_states):\n",
    "        if A[i].sum() == 0:\n",
    "            A[i,i] = 1.0\n",
    "\n",
    "    # emission Gaussians\n",
    "    means, covars = [], []\n",
    "    reg = 1e-2\n",
    "    for i in range(n_states):\n",
    "        obs = X[:, y_idx==i].T\n",
    "        mu  = obs.mean(axis=0)\n",
    "        C   = np.cov(obs, rowvar=False)\n",
    "        C   = 0.5*(C + C.T) + reg*np.eye(C.shape[1])\n",
    "        means.append(mu); covars.append(C)\n",
    "    means  = np.vstack(means)\n",
    "    covars = np.stack(covars)\n",
    "\n",
    "    # --- 6) Build HMM & decode via Viterbi ---\n",
    "    model = hmm.GaussianHMM(n_components=n_states,\n",
    "                            covariance_type=\"full\",\n",
    "                            init_params=\"\")\n",
    "    model.startprob_ = pi\n",
    "    model.transmat_  = A\n",
    "    model.means_     = means\n",
    "    model.covars_    = covars\n",
    "\n",
    "    _, y_pred_idx = model.decode(X.T, algorithm=\"viterbi\")\n",
    "    y_pred = np.array([idx2state[i] for i in y_pred_idx])\n",
    "\n",
    "    # accuracy\n",
    "    acc = (y_pred == y_true).mean()\n",
    "    print(f\"{subj_id}: Viterbi accuracy = {acc*100:.2f}%\")\n",
    "\n",
    "    # --- 7) Plot true vs predicted states ---\n",
    "    state_y  = {s:i for i,s in enumerate(states)}\n",
    "    y_true_i = [state_y[s] for s in y_true]\n",
    "    y_pred_i = [state_y[s] for s in y_pred]\n",
    "\n",
    "    fig2, ax2 = plt.subplots(figsize=(12,3))\n",
    "    ax2.step(times, y_true_i, where=\"post\", linestyle=\"--\", label=\"True\",   lw=1)\n",
    "    ax2.step(times, y_pred_i, where=\"post\", linestyle=\"-\",  label=\"Predicted\", lw=1.5)\n",
    "    ax2.set_yticks(list(state_y.values())); ax2.set_yticklabels(states)\n",
    "    ax2.set_title(f\"{subj_id}: True vs Predicted (acc {acc*100:.1f}%)\")\n",
    "    ax2.set_xlabel(\"Time\"); ax2.grid(axis=\"x\", linestyle=\"--\", alpha=0.3)\n",
    "    ax2.legend(loc=\"upper right\", fontsize=\"small\")\n",
    "    fig2.autofmt_xdate()\n",
    "    plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from hmmlearn import hmm\n",
    "\n",
    "# === User settings ===\n",
    "root_dir = \"/Users/kyrusmama/Downloads/ref_prep_synchr_MAT_jiahui/Data_MGR\"  # contains sub-1, sub-2, etc.\n",
    "selected_features = [\n",
    "  \"Delta_TP9\", \"Delta_AF7\", \"Delta_AF8\", \"Delta_TP10\",\n",
    "  \"Theta_TP9\", \"Theta_AF7\", \"Theta_AF8\", \"Theta_TP10\",\n",
    "  \"Alpha_TP9\", \"Alpha_AF7\", \"Alpha_AF8\", \"Alpha_TP10\",\n",
    "  \"Beta_TP9\",  \"Beta_AF7\",  \"Beta_AF8\",  \"Beta_TP10\",\n",
    "  # \"Gamma_TP9\", \"Gamma_AF7\", \"Gamma_AF8\", \"Gamma_TP10\"\n",
    "]\n",
    "\n",
    "# find all subject folders\n",
    "subject_dirs = sorted(glob.glob(os.path.join(root_dir, \"sub-*\")))\n",
    "\n",
    "for subj_dir in subject_dirs:\n",
    "    subj_id = os.path.basename(subj_dir)\n",
    "    # locate EEG CSV and log file\n",
    "    eeg_files = glob.glob(os.path.join(subj_dir, \"*mindMonitor*.csv\"))\n",
    "    log_files = glob.glob(os.path.join(subj_dir, \"*music_genre_experiment_log*.txt\"))\n",
    "    if not eeg_files or not log_files:\n",
    "        print(f\"{subj_id}: missing data or log → skipping\")\n",
    "        continue\n",
    "    eeg_file, log_file = eeg_files[0], log_files[0]\n",
    "\n",
    "    # --- 1) Load & clean EEG data ---\n",
    "    df = pd.read_csv(eeg_file)\n",
    "    if \"Elements\" in df.columns:\n",
    "        df = df[df[\"Elements\"].isna()]\n",
    "    df[\"Time\"] = pd.to_datetime(df[\"TimeStamp\"])\n",
    "    df.set_index(\"Time\", inplace=True)\n",
    "\n",
    "    # --- 2) Parse log & build intervals ---\n",
    "    sync_df = pd.read_csv(log_file, sep=\"\\t\", header=None, names=[\"Event\",\"Time\"])\n",
    "    sync_df.dropna(inplace=True)\n",
    "    exp0 = float(sync_df.loc[sync_df[\"Event\"].str.contains(\"Experiment Unix Start\"), \"Time\"].iat[0])\n",
    "    t0   = exp0 + 2*3600\n",
    "    to_ts = lambda offs: pd.to_datetime(t0 + float(offs), unit=\"s\")\n",
    "\n",
    "    intervals = {\"Rest\": [], \"Jazz\": [], \"Classical\": [], \"Electronic\": []}\n",
    "    # baseline → Rest\n",
    "    b0 = sync_df.loc[sync_df[\"Event\"]==\"Baseline Start\", \"Time\"].iat[0]\n",
    "    b1 = sync_df.loc[sync_df[\"Event\"]==\"Baseline End\",   \"Time\"].iat[0]\n",
    "    intervals[\"Rest\"].append((to_ts(b0), to_ts(b1)))\n",
    "    # music & post‐rest\n",
    "    for i,row in sync_df.iterrows():\n",
    "        ev, offs = row[\"Event\"], row[\"Time\"]\n",
    "        if ev.startswith(\"Music Start\"):\n",
    "            genre = ev.split(\" - \")[2]\n",
    "            start = to_ts(offs)\n",
    "            end   = to_ts(sync_df.at[i+1, \"Time\"])\n",
    "            intervals[genre].append((start, end))\n",
    "        elif ev.startswith(\"Post Rest Start\"):\n",
    "            start = to_ts(offs)\n",
    "            end   = to_ts(sync_df.at[i+1, \"Time\"])\n",
    "            intervals[\"Rest\"].append((start, end))\n",
    "\n",
    "    # --- 3) Label each EEG sample ---\n",
    "    df[\"Condition\"] = \"Unlabeled\"\n",
    "    for cond, spans in intervals.items():\n",
    "        for start, end in spans:\n",
    "            df.loc[(df.index >= start) & (df.index <= end), \"Condition\"] = cond\n",
    "    df = df[df[\"Condition\"] != \"Unlabeled\"]\n",
    "\n",
    "    # --- 4) Select features & build arrays ---\n",
    "    feature_cols = [c for c in selected_features if c in df.columns]\n",
    "    if not feature_cols:\n",
    "        feature_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "    X      = df[feature_cols].T.values      # (n_features, n_timesteps)\n",
    "    y_true = df[\"Condition\"].values         # (n_timesteps,)\n",
    "    times  = df.index.to_numpy()            # (n_timesteps,)\n",
    "\n",
    "    # determine split index at 75% of timeline\n",
    "    split_idx = int(0.75 * len(times))\n",
    "    # training data\n",
    "    X_train    = X[:, :split_idx]\n",
    "    y_train    = y_true[:split_idx]\n",
    "    # test data\n",
    "    X_test     = X[:, split_idx:]\n",
    "    y_test     = y_true[split_idx:]\n",
    "    times_train = times[:split_idx]\n",
    "    times_test  = times[split_idx:]\n",
    "\n",
    "    # --- 5) Estimate HMM parameters on training set ---\n",
    "    states    = sorted(np.unique(y_train))\n",
    "    state2idx = {s:i for i,s in enumerate(states)}\n",
    "    idx2state = {i:s for s,i in state2idx.items()}\n",
    "    y_train_idx = np.array([state2idx[s] for s in y_train])\n",
    "    n_states  = len(states)\n",
    "\n",
    "    # initial π\n",
    "    pi = np.zeros(n_states)\n",
    "    pi[y_train_idx[0]] = 1.0\n",
    "\n",
    "    # transition matrix A\n",
    "    A_counts = np.zeros((n_states, n_states))\n",
    "    for a,b in zip(y_train_idx[:-1], y_train_idx[1:]):\n",
    "        A_counts[a,b] += 1\n",
    "    A = (A_counts.T / A_counts.sum(axis=1)).T\n",
    "    for i in range(n_states):\n",
    "        if A[i].sum() == 0:\n",
    "            A[i,i] = 1.0\n",
    "\n",
    "    # emission Gaussians\n",
    "    means, covars = [], []\n",
    "    reg = 1e-2\n",
    "    for i in range(n_states):\n",
    "        obs = X_train[:, y_train_idx==i].T\n",
    "        mu  = obs.mean(axis=0)\n",
    "        C   = np.cov(obs, rowvar=False)\n",
    "        C   = 0.5*(C + C.T) + reg*np.eye(C.shape[1])\n",
    "        means.append(mu)\n",
    "        covars.append(C)\n",
    "    means  = np.vstack(means)\n",
    "    covars = np.stack(covars)\n",
    "\n",
    "    # build HMM\n",
    "    model = hmm.GaussianHMM(n_components=n_states,\n",
    "                            covariance_type=\"full\",\n",
    "                            init_params=\"\")\n",
    "    model.startprob_ = pi\n",
    "    model.transmat_  = A\n",
    "    model.means_     = means\n",
    "    model.covars_    = covars\n",
    "\n",
    "    # --- 6) Decode train & test via Viterbi ---\n",
    "    _, y_pred_train_idx = model.decode(X_train.T, algorithm=\"viterbi\")\n",
    "    _, y_pred_test_idx  = model.decode(X_test.T,  algorithm=\"viterbi\")\n",
    "\n",
    "    # convert to labels\n",
    "    y_pred_train = [idx2state[i] for i in y_pred_train_idx]\n",
    "    y_pred_test  = [idx2state[i] for i in y_pred_test_idx]\n",
    "\n",
    "    # compute accuracies\n",
    "    acc_train = np.mean(np.array(y_pred_train) == y_train)\n",
    "    acc_test  = np.mean(np.array(y_pred_test)  == y_test)\n",
    "    print(f\"{subj_id}: train acc = {acc_train*100:.1f}%, test acc = {acc_test*100:.1f}%\")\n",
    "\n",
    "    # --- 7) Plot true vs predicted, distinguishing train/test ---\n",
    "    state_y = {s:i for i,s in enumerate(states)}\n",
    "    # convert true labels to ints for plotting\n",
    "    y_train_i = [state_y[s] for s in y_train]\n",
    "    y_test_i  = [state_y[s] for s in y_test]\n",
    "    y_pred_train_i = [state_y[s] for s in y_pred_train]\n",
    "    y_pred_test_i  = [state_y[s] for s in y_pred_test]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12,4))\n",
    "    # training segment\n",
    "    ax.step(times_train, y_train_i,    where=\"post\",\n",
    "            linestyle=\"--\", color=\"black\", label=\"True (train)\")\n",
    "    ax.step(times_train, y_pred_train_i,where=\"post\",\n",
    "            linestyle=\"-\",  color=\"blue\",  label=\"Pred (train)\")\n",
    "    # testing segment\n",
    "    ax.step(times_test, y_test_i,      where=\"post\",\n",
    "            linestyle=\"--\", color=\"gray\", label=\"True (test)\")\n",
    "    ax.step(times_test, y_pred_test_i, where=\"post\",\n",
    "            linestyle=\"-\",  color=\"red\",  label=\"Pred (test)\")\n",
    "\n",
    "    ax.set_yticks(list(state_y.values()))\n",
    "    ax.set_yticklabels(states)\n",
    "    ax.set_xlabel(\"Time\")\n",
    "    ax.set_title(f\"{subj_id}: Train vs Test Decoding\")\n",
    "    ax.legend(loc=\"upper right\", fontsize=\"small\")\n",
    "    ax.grid(axis=\"x\", linestyle=\"--\", alpha=0.3)\n",
    "    fig.autofmt_xdate()\n",
    "    plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from hmmlearn import hmm\n",
    "\n",
    "# === User settings ===\n",
    "root_dir = \"/Users/kyrusmama/Downloads/ref_prep_synchr_MAT_jiahui/Data_MGR\"\n",
    "selected_features = [\n",
    "  \"Delta_TP9\", \"Delta_AF7\", \"Delta_AF8\", \"Delta_TP10\",\n",
    "  \"Theta_TP9\", \"Theta_AF7\", \"Theta_AF8\", \"Theta_TP10\",\n",
    "  \"Alpha_TP9\", \"Alpha_AF7\", \"Alpha_AF8\", \"Alpha_TP10\",\n",
    "  \"Beta_TP9\",  \"Beta_AF7\",  \"Beta_AF8\",  \"Beta_TP10\"\n",
    "]\n",
    "\n",
    "# Now only two states: Rest vs Music\n",
    "states = [\"Rest\", \"Music\"]\n",
    "state2idx = {s:i for i,s in enumerate(states)}\n",
    "idx2state = {i:s for s,i in state2idx.items()}\n",
    "n_states = len(states)\n",
    "\n",
    "# find all subject folders\n",
    "subject_dirs = sorted(glob.glob(os.path.join(root_dir, \"sub-*\")))\n",
    "\n",
    "for subj_dir in subject_dirs:\n",
    "    subj_id = os.path.basename(subj_dir)\n",
    "    eeg_files = glob.glob(os.path.join(subj_dir, \"*mindMonitor*.csv\"))\n",
    "    log_files = glob.glob(os.path.join(subj_dir, \"*music_genre_experiment_log*.txt\"))\n",
    "    if not eeg_files or not log_files:\n",
    "        print(f\"{subj_id}: missing data or log → skipping\")\n",
    "        continue\n",
    "    eeg_file, log_file = eeg_files[0], log_files[0]\n",
    "\n",
    "    # --- 1) Load & clean EEG data ---\n",
    "    df = pd.read_csv(eeg_file)\n",
    "    if \"Elements\" in df.columns:\n",
    "        df = df[df[\"Elements\"].isna()]\n",
    "    df[\"Time\"] = pd.to_datetime(df[\"TimeStamp\"])\n",
    "    df.set_index(\"Time\", inplace=True)\n",
    "\n",
    "    # --- 2) Parse log & build intervals ---\n",
    "    sync_df = pd.read_csv(log_file, sep=\"\\t\", header=None, names=[\"Event\",\"Time\"])\n",
    "    sync_df.dropna(inplace=True)\n",
    "    exp0 = float(sync_df.loc[sync_df[\"Event\"].str.contains(\"Experiment Unix Start\"), \"Time\"].iat[0])\n",
    "    t0   = exp0 + 2*3600\n",
    "    to_ts = lambda offs: pd.to_datetime(t0 + float(offs), unit=\"s\")\n",
    "\n",
    "    # original intervals for Rest and all music genres\n",
    "    intervals = {\"Rest\": [], \"Jazz\": [], \"Classical\": [], \"Electronic\": []}\n",
    "    # baseline → Rest\n",
    "    b0 = sync_df.loc[sync_df[\"Event\"]==\"Baseline Start\", \"Time\"].iat[0]\n",
    "    b1 = sync_df.loc[sync_df[\"Event\"]==\"Baseline End\",   \"Time\"].iat[0]\n",
    "    intervals[\"Rest\"].append((to_ts(b0), to_ts(b1)))\n",
    "    # music & post‐rest\n",
    "    for i,row in sync_df.iterrows():\n",
    "        ev, offs = row[\"Event\"], row[\"Time\"]\n",
    "        if ev.startswith(\"Music Start\"):\n",
    "            genre = ev.split(\" - \")[2]\n",
    "            start = to_ts(offs)\n",
    "            end   = to_ts(sync_df.at[i+1, \"Time\"])\n",
    "            intervals[genre].append((start, end))\n",
    "        elif ev.startswith(\"Post Rest Start\"):\n",
    "            start = to_ts(offs)\n",
    "            end   = to_ts(sync_df.at[i+1, \"Time\"])\n",
    "            intervals[\"Rest\"].append((start, end))\n",
    "\n",
    "    # --- 3) Label each EEG sample, then map to binary ---\n",
    "    df[\"Condition\"] = \"Unlabeled\"\n",
    "    for cond, spans in intervals.items():\n",
    "        for start, end in spans:\n",
    "            df.loc[(df.index>=start)&(df.index<=end), \"Condition\"] = cond\n",
    "    df = df[df[\"Condition\"]!=\"Unlabeled\"]\n",
    "    # now collapse all music genres into \"Music\"\n",
    "    df[\"Condition\"] = df[\"Condition\"].apply(lambda c: \"Rest\" if c==\"Rest\" else \"Music\")\n",
    "\n",
    "    # --- 4) Select features & build arrays ---\n",
    "    feature_cols = [c for c in selected_features if c in df.columns]\n",
    "    if not feature_cols:\n",
    "        feature_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "    X      = df[feature_cols].T.values      # (n_features, n_timesteps)\n",
    "    y_true = df[\"Condition\"].values         # (n_timesteps,)\n",
    "    times  = df.index.to_numpy()            # (n_timesteps,)\n",
    "\n",
    "    # --- 5) Train/test split at 75% time ---\n",
    "    split = int(0.75 * len(times))\n",
    "    X_train, X_test = X[:, :split], X[:, split:]\n",
    "    y_train, y_test = y_true[:split],   y_true[split:]\n",
    "    t_train, t_test = times[:split],    times[split:]\n",
    "\n",
    "    # encode\n",
    "    y_train_idx = np.array([state2idx[c] for c in y_train])\n",
    "    y_test_idx  = np.array([state2idx[c] for c in y_test])\n",
    "\n",
    "    # --- 6) Estimate HMM on training set ---\n",
    "    # π\n",
    "    pi = np.zeros(n_states); pi[y_train_idx[0]] = 1.0\n",
    "    # A\n",
    "    A_counts = np.zeros((n_states, n_states))\n",
    "    for a,b in zip(y_train_idx[:-1], y_train_idx[1:]):\n",
    "        A_counts[a,b] += 1\n",
    "    A = (A_counts.T / A_counts.sum(axis=1)).T\n",
    "    for i in range(n_states):\n",
    "        if A[i].sum()==0: A[i,i]=1.0\n",
    "\n",
    "    # emissions\n",
    "    means, covars = [], []\n",
    "    reg = 1e-2\n",
    "    for i in range(n_states):\n",
    "        obs = X_train[:, y_train_idx==i].T\n",
    "        mu  = obs.mean(axis=0)\n",
    "        C   = np.cov(obs, rowvar=False)\n",
    "        C   = 0.5*(C + C.T) + reg*np.eye(C.shape[1])\n",
    "        means.append(mu); covars.append(C)\n",
    "    means  = np.vstack(means)\n",
    "    covars = np.stack(covars)\n",
    "\n",
    "    # build HMM\n",
    "    model = hmm.GaussianHMM(n_components=n_states,\n",
    "                            covariance_type=\"full\",\n",
    "                            init_params=\"\")\n",
    "    model.startprob_, model.transmat_, model.means_, model.covars_ = pi, A, means, covars\n",
    "\n",
    "    # --- 7) Decode train & test ---\n",
    "    _, y_pred_train_idx = model.decode(X_train.T, algorithm=\"viterbi\")\n",
    "    _, y_pred_test_idx  = model.decode(X_test.T,  algorithm=\"viterbi\")\n",
    "    y_pred_train = [idx2state[i] for i in y_pred_train_idx]\n",
    "    y_pred_test  = [idx2state[i] for i in y_pred_test_idx]\n",
    "\n",
    "    acc_train = np.mean(np.array(y_pred_train) == y_train)\n",
    "    acc_test  = np.mean(np.array(y_pred_test)  == y_test)\n",
    "    print(f\"{subj_id}: train acc = {acc_train*100:.1f}%, test acc = {acc_test*100:.1f}%\")\n",
    "\n",
    "    # --- 8) Plot results, distinguishing train vs test ---\n",
    "    state_y = {s:i for i,s in enumerate(states)}\n",
    "    y_tr_i  = [state_y[s] for s in y_train]\n",
    "    y_te_i  = [state_y[s] for s in y_test]\n",
    "    y_pt_i  = [state_y[s] for s in y_pred_train]\n",
    "    y_pt2_i = [state_y[s] for s in y_pred_test]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12,4))\n",
    "    # train\n",
    "    ax.step(t_train, y_tr_i,    where=\"post\", linestyle=\"--\", color=\"black\", label=\"True (train)\")\n",
    "    ax.step(t_train, y_pt_i,    where=\"post\", linestyle=\"-\",  color=\"blue\",  label=\"Pred (train)\")\n",
    "    # test\n",
    "    ax.step(t_test,  y_te_i,    where=\"post\", linestyle=\"--\", color=\"gray\",  label=\"True (test)\")\n",
    "    ax.step(t_test,  y_pt2_i,   where=\"post\", linestyle=\"-\",  color=\"red\",   label=\"Pred (test)\")\n",
    "\n",
    "    ax.set_yticks([0,1])\n",
    "    ax.set_yticklabels(states)\n",
    "    ax.set_xlabel(\"Time\")\n",
    "    ax.set_title(f\"{subj_id}: Music vs Rest ({acc_test*100:.1f}% test acc)\")\n",
    "    ax.legend(loc=\"upper right\", fontsize=\"small\")\n",
    "    ax.grid(axis=\"x\", linestyle=\"--\", alpha=0.3)\n",
    "    fig.autofmt_xdate()\n",
    "    plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from hmmlearn import hmm\n",
    "\n",
    "# === User settings ===\n",
    "root_dir = \"/Users/kyrusmama/Downloads/ref_prep_synchr_MAT_jiahui/Data_MGR\"  # contains sub-1, sub-2, etc.\n",
    "# List specific EEG feature columns to use, or leave empty to use all numeric columns:\n",
    "selected_features = [\n",
    "  \"Delta_TP9\", \"Delta_AF7\", \"Delta_AF8\", \"Delta_TP10\",\n",
    "  \"Theta_TP9\", \"Theta_AF7\", \"Theta_AF8\", \"Theta_TP10\",\n",
    "  \"Alpha_TP9\", \"Alpha_AF7\", \"Alpha_AF8\", \"Alpha_TP10\",\n",
    "  \"Beta_TP9\",  \"Beta_AF7\",  \"Beta_AF8\",  \"Beta_TP10\",\n",
    "  \"Gamma_TP9\", \"Gamma_AF7\", \"Gamma_AF8\", \"Gamma_TP10\"\n",
    "]\n",
    "\n",
    "\n",
    "# These are the four states we’ll classify:\n",
    "all_states = [\"Rest\", \"Jazz\", \"Classical\", \"Electronic\"]\n",
    "n_states   = len(all_states)\n",
    "state2idx  = {s:i for i,s in enumerate(all_states)}\n",
    "idx2state  = {i:s for s,i in state2idx.items()}\n",
    "\n",
    "# Containers for global training\n",
    "init_counts    = np.zeros(n_states)               # counts of which state subjects start in\n",
    "trans_counts   = np.zeros((n_states, n_states))   # global transition counts\n",
    "emission_data  = {s: [] for s in all_states}      # list of observations per state\n",
    "subject_data   = {}                               # to store each subject's (X, y_idx, times)\n",
    "\n",
    "# === PHASE 1: Collect data & accumulate statistics ===\n",
    "for subj_dir in sorted(glob.glob(os.path.join(root_dir, \"sub-*\"))):\n",
    "    subj_id = os.path.basename(subj_dir)\n",
    "    # find files\n",
    "    eeg_files = glob.glob(os.path.join(subj_dir, \"*mindMonitor*.csv\"))\n",
    "    log_files = glob.glob(os.path.join(subj_dir, \"*music_genre_experiment_log*.txt\"))\n",
    "    if not eeg_files or not log_files:\n",
    "        print(f\"{subj_id}: missing data, skipping\")\n",
    "        continue\n",
    "    eeg_file, log_file = eeg_files[0], log_files[0]\n",
    "\n",
    "    # 1) load & clean EEG\n",
    "    df = pd.read_csv(eeg_file)\n",
    "    if \"Elements\" in df.columns:\n",
    "        df = df[df[\"Elements\"].isna()]\n",
    "    df[\"Time\"] = pd.to_datetime(df[\"TimeStamp\"])\n",
    "    df.set_index(\"Time\", inplace=True)\n",
    "\n",
    "    # 2) parse log and build intervals\n",
    "    sync_df = pd.read_csv(log_file, sep=\"\\t\", header=None, names=[\"Event\",\"Time\"])\n",
    "    sync_df.dropna(inplace=True)\n",
    "    exp0 = float(sync_df.loc[sync_df[\"Event\"].str.contains(\"Experiment Unix Start\"), \"Time\"].iat[0])\n",
    "    t0   = exp0 + 2*3600\n",
    "    to_ts = lambda offs: pd.to_datetime(t0 + float(offs), unit=\"s\")\n",
    "\n",
    "    intervals = {s: [] for s in all_states}\n",
    "    # baseline → Rest\n",
    "    b0 = sync_df.loc[sync_df[\"Event\"]==\"Baseline Start\", \"Time\"].iat[0]\n",
    "    b1 = sync_df.loc[sync_df[\"Event\"]==\"Baseline End\",   \"Time\"].iat[0]\n",
    "    intervals[\"Rest\"].append((to_ts(b0), to_ts(b1)))\n",
    "    # music & post-rest\n",
    "    for i,row in sync_df.iterrows():\n",
    "        ev, offs = row[\"Event\"], row[\"Time\"]\n",
    "        if ev.startswith(\"Music Start\"):\n",
    "            genre = ev.split(\" - \")[2]\n",
    "            start = to_ts(offs)\n",
    "            end   = to_ts(sync_df.at[i+1, \"Time\"])\n",
    "            intervals[genre].append((start, end))\n",
    "        elif ev.startswith(\"Post Rest Start\"):\n",
    "            start = to_ts(offs)\n",
    "            end   = to_ts(sync_df.at[i+1, \"Time\"])\n",
    "            intervals[\"Rest\"].append((start, end))\n",
    "\n",
    "    # 3) label each sample\n",
    "    df[\"Condition\"] = \"Unlabeled\"\n",
    "    for cond, spans in intervals.items():\n",
    "        for start,end in spans:\n",
    "            df.loc[(df.index>=start)&(df.index<=end),\"Condition\"] = cond\n",
    "    df = df[df[\"Condition\"]!=\"Unlabeled\"]\n",
    "\n",
    "    # 4) select features\n",
    "    if selected_features:\n",
    "        missing = set(selected_features) - set(df.columns)\n",
    "        if missing:\n",
    "            print(f\"{subj_id}: warning, missing {missing}\")\n",
    "        feature_cols = [c for c in selected_features if c in df.columns]\n",
    "    else:\n",
    "        feature_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "    X      = df[feature_cols].T.values      # (n_features, n_timesteps)\n",
    "    conds  = df[\"Condition\"].values\n",
    "    times  = df.index.to_numpy()\n",
    "\n",
    "    # convert conds to indices\n",
    "    y_idx = np.array([state2idx[c] for c in conds])\n",
    "\n",
    "    # accumulate init count\n",
    "    init_counts[y_idx[0]] += 1\n",
    "\n",
    "    # accumulate transitions\n",
    "    for a,b in zip(y_idx[:-1], y_idx[1:]):\n",
    "        trans_counts[a,b] += 1\n",
    "\n",
    "    # collect emissions\n",
    "    for i in range(n_states):\n",
    "        mask = (y_idx == i)\n",
    "        if mask.any():\n",
    "            emission_data[ idx2state[i] ].append( X[:,mask].T )\n",
    "\n",
    "    # store for later evaluation\n",
    "    subject_data[subj_id] = (X, y_idx, times)\n",
    "\n",
    "# build global pi\n",
    "pi = init_counts / init_counts.sum()\n",
    "\n",
    "# build global A\n",
    "A = (trans_counts.T / trans_counts.sum(axis=1)).T\n",
    "for i in range(n_states):\n",
    "    if A[i].sum()==0:\n",
    "        A[i,i] = 1.0\n",
    "\n",
    "# build global emission Gaussians\n",
    "means, covars = [], []\n",
    "reg = 1e-2\n",
    "for state in all_states:\n",
    "    # concatenate all obs for this state\n",
    "    data = np.vstack(emission_data[state])  # shape (total_samples, n_features)\n",
    "    mu = data.mean(axis=0)\n",
    "    C  = np.cov(data, rowvar=False)\n",
    "    C  = 0.5*(C + C.T) + reg*np.eye(C.shape[1])\n",
    "    means.append(mu)\n",
    "    covars.append(C)\n",
    "means  = np.vstack(means)\n",
    "covars = np.stack(covars)\n",
    "\n",
    "# build the global HMM\n",
    "model = hmm.GaussianHMM(n_components=n_states,\n",
    "                        covariance_type=\"full\",\n",
    "                        init_params=\"\")\n",
    "model.startprob_ = pi\n",
    "model.transmat_  = A\n",
    "model.means_     = means\n",
    "model.covars_    = covars\n",
    "\n",
    "# === PHASE 2: Apply to each subject & report ===\n",
    "for subj_id, (X, y_idx_true, times) in subject_data.items():\n",
    "    # decode\n",
    "    _, y_pred_idx = model.decode(X.T, algorithm=\"viterbi\")\n",
    "    acc = (y_pred_idx == y_idx_true).mean()\n",
    "    print(f\"{subj_id}: global‐model accuracy = {acc*100:.2f}%\")\n",
    "\n",
    "    # plot true vs predicted\n",
    "    fig, ax = plt.subplots(figsize=(10,3))\n",
    "    ax.step(times, y_idx_true, where=\"post\", linestyle=\"--\", label=\"True\",   lw=1)\n",
    "    ax.step(times, y_pred_idx, where=\"post\", linestyle=\"-\",  label=\"Pred.\",   lw=1.5)\n",
    "    ax.set_yticks(range(n_states)); ax.set_yticklabels(all_states)\n",
    "    ax.set_title(f\"{subj_id}: True vs. Global‐Model Pred (acc {acc*100:.1f}%)\")\n",
    "    ax.set_xlabel(\"Time\"); ax.grid(axis=\"x\", linestyle=\"--\", alpha=0.3)\n",
    "    ax.legend(loc=\"upper right\", fontsize=\"small\")\n",
    "    fig.autofmt_xdate()\n",
    "    plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from hmmlearn import hmm\n",
    "\n",
    "# === User settings ===\n",
    "root_dir = \"/Users/kyrusmama/Downloads/ref_prep_synchr_MAT_jiahui/Data_MGR\"\n",
    "selected_features = [\n",
    "  \"Delta_TP9\", \"Delta_AF7\", \"Delta_AF8\", \"Delta_TP10\",\n",
    "  \"Theta_TP9\", \"Theta_AF7\", \"Theta_AF8\", \"Theta_TP10\",\n",
    "  \"Alpha_TP9\", \"Alpha_AF7\", \"Alpha_AF8\", \"Alpha_TP10\",\n",
    "  \"Beta_TP9\",  \"Beta_AF7\",  \"Beta_AF8\",  \"Beta_TP10\",\n",
    "  \"Gamma_TP9\", \"Gamma_AF7\", \"Gamma_AF8\", \"Gamma_TP10\"\n",
    "]\n",
    "\n",
    "# The four states\n",
    "all_states = [\"Rest\", \"Jazz\", \"Classical\", \"Electronic\"]\n",
    "n_states   = len(all_states)\n",
    "state2idx  = {s:i for i,s in enumerate(all_states)}\n",
    "idx2state  = {i:s for s,i in state2idx.items()}\n",
    "\n",
    "# Helper to load one subject's data\n",
    "def load_subject(subj_dir):\n",
    "    eeg_files = glob.glob(os.path.join(subj_dir, \"*mindMonitor*.csv\"))\n",
    "    log_files = glob.glob(os.path.join(subj_dir, \"*music_genre_experiment_log*.txt\"))\n",
    "    if not eeg_files or not log_files:\n",
    "        return None\n",
    "    # --- Load & clean EEG ---\n",
    "    df = pd.read_csv(eeg_files[0])\n",
    "    if \"Elements\" in df.columns:\n",
    "        df = df[df[\"Elements\"].isna()]\n",
    "    df[\"Time\"] = pd.to_datetime(df[\"TimeStamp\"])\n",
    "    df.set_index(\"Time\", inplace=True)\n",
    "    # --- Parse log & build intervals ---\n",
    "    sync_df = pd.read_csv(log_files[0], sep=\"\\t\",\n",
    "                          header=None, names=[\"Event\",\"Time\"])\n",
    "    sync_df.dropna(inplace=True)\n",
    "    exp0 = float(sync_df.loc[\n",
    "        sync_df[\"Event\"].str.contains(\"Experiment Unix Start\"), \"Time\"].iat[0])\n",
    "    t0 = exp0 + 2*3600\n",
    "    to_ts = lambda offs: pd.to_datetime(t0 + float(offs), unit=\"s\")\n",
    "    intervals = {s: [] for s in all_states}\n",
    "    # Baseline → Rest\n",
    "    b0 = sync_df.loc[sync_df[\"Event\"]==\"Baseline Start\", \"Time\"].iat[0]\n",
    "    b1 = sync_df.loc[sync_df[\"Event\"]==\"Baseline End\",   \"Time\"].iat[0]\n",
    "    intervals[\"Rest\"].append((to_ts(b0), to_ts(b1)))\n",
    "    # Music & Post‐Rest\n",
    "    for i,row in sync_df.iterrows():\n",
    "        ev, offs = row[\"Event\"], row[\"Time\"]\n",
    "        if ev.startswith(\"Music Start\"):\n",
    "            genre = ev.split(\" - \")[2]\n",
    "            start = to_ts(offs)\n",
    "            end   = to_ts(sync_df.at[i+1, \"Time\"])\n",
    "            intervals[genre].append((start, end))\n",
    "        elif ev.startswith(\"Post Rest Start\"):\n",
    "            start = to_ts(offs)\n",
    "            end   = to_ts(sync_df.at[i+1, \"Time\"])\n",
    "            intervals[\"Rest\"].append((start, end))\n",
    "    # --- Label each sample ---\n",
    "    df[\"Condition\"] = \"Unlabeled\"\n",
    "    for cond, spans in intervals.items():\n",
    "        for start,end in spans:\n",
    "            df.loc[(df.index>=start)&(df.index<=end), \"Condition\"] = cond\n",
    "    df = df[df[\"Condition\"]!=\"Unlabeled\"]\n",
    "    # --- Select features & build arrays ---\n",
    "    feature_cols = [c for c in selected_features if c in df.columns]\n",
    "    if not feature_cols:\n",
    "        feature_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    X      = df[feature_cols].T.values      # shape (n_features, n_timesteps)\n",
    "    y_idx  = np.array([state2idx[c] for c in df[\"Condition\"].values])\n",
    "    times  = df.index.to_numpy()\n",
    "    return X, y_idx, times\n",
    "\n",
    "# === Load all subjects ===\n",
    "subject_dirs = sorted(glob.glob(os.path.join(root_dir, \"sub-*\")))\n",
    "subject_data = {}\n",
    "for sd in subject_dirs:\n",
    "    sid = os.path.basename(sd)\n",
    "    data = load_subject(sd)\n",
    "    if data is not None:\n",
    "        subject_data[sid] = data\n",
    "    else:\n",
    "        print(f\"{sid}: missing files, skipping\")\n",
    "\n",
    "# === Leave-One-Subject-Out Cross-Validation ===\n",
    "for left_out in subject_data:\n",
    "    # --- 1) Accumulate HMM stats from all OTHER subjects ---\n",
    "    init_counts   = np.zeros(n_states)\n",
    "    trans_counts  = np.zeros((n_states,n_states))\n",
    "    emission_data = {s: [] for s in all_states}\n",
    "\n",
    "    for sid, (X, y_idx, _) in subject_data.items():\n",
    "        if sid == left_out:\n",
    "            continue\n",
    "        # initial state\n",
    "        init_counts[y_idx[0]] += 1\n",
    "        # transitions\n",
    "        for a,b in zip(y_idx[:-1], y_idx[1:]):\n",
    "            trans_counts[a,b] += 1\n",
    "        # emissions\n",
    "        for state in all_states:\n",
    "            i = state2idx[state]\n",
    "            mask = (y_idx == i)\n",
    "            if mask.any():\n",
    "                emission_data[state].append(X[:, mask].T)\n",
    "\n",
    "    # normalize π\n",
    "    pi = init_counts / init_counts.sum()\n",
    "\n",
    "    # normalize A\n",
    "    A = (trans_counts.T / trans_counts.sum(axis=1)).T\n",
    "    for i in range(n_states):\n",
    "        if A[i].sum() == 0:\n",
    "            A[i,i] = 1.0\n",
    "\n",
    "    # fit Gaussian emissions\n",
    "    means, covars = [], []\n",
    "    reg = 1e-2\n",
    "    for state in all_states:\n",
    "        data = np.vstack(emission_data[state])  # (n_samples, n_features)\n",
    "        mu = data.mean(axis=0)\n",
    "        C  = np.cov(data, rowvar=False)\n",
    "        C  = 0.5*(C + C.T) + reg*np.eye(C.shape[1])\n",
    "        means.append(mu)\n",
    "        covars.append(C)\n",
    "    means  = np.vstack(means)\n",
    "    covars = np.stack(covars)\n",
    "\n",
    "    # build HMM\n",
    "    model = hmm.GaussianHMM(n_components=n_states,\n",
    "                            covariance_type=\"full\",\n",
    "                            init_params=\"\")\n",
    "    model.startprob_ = pi\n",
    "    model.transmat_  = A\n",
    "    model.means_     = means\n",
    "    model.covars_    = covars\n",
    "\n",
    "    # --- 2) Test on the left-out subject ---\n",
    "    X_test, y_idx_test, times_test = subject_data[left_out]\n",
    "    _, y_pred_idx = model.decode(X_test.T, algorithm=\"viterbi\")\n",
    "    acc = (y_pred_idx == y_idx_test).mean()\n",
    "    print(f\"{left_out}: LOO accuracy = {acc*100:.2f}%\")\n",
    "\n",
    "    # --- 3) Plot true vs predicted for this subject ---\n",
    "    fig, ax = plt.subplots(figsize=(10,3))\n",
    "    ax.step(times_test, y_idx_test,    where=\"post\",\n",
    "            linestyle=\"--\", label=\"True\",   lw=1)\n",
    "    ax.step(times_test, y_pred_idx,    where=\"post\",\n",
    "            linestyle=\"-\",  label=\"Predicted\", lw=1.5)\n",
    "    ax.set_yticks(range(n_states))\n",
    "    ax.set_yticklabels(all_states)\n",
    "    ax.set_title(f\"{left_out}: True vs Predicted (LOO acc {acc*100:.1f}%)\")\n",
    "    ax.set_xlabel(\"Time\")\n",
    "    ax.grid(axis=\"x\", linestyle=\"--\", alpha=0.3)\n",
    "    ax.legend(loc=\"upper right\", fontsize=\"small\")\n",
    "    fig.autofmt_xdate()\n",
    "    plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from hmmlearn import hmm\n",
    "\n",
    "# === User settings ===\n",
    "root_dir = \"/Users/kyrusmama/Downloads/ref_prep_synchr_MAT_jiahui/Data_MGR\"\n",
    "selected_features = [\n",
    "  \"Delta_TP9\", \"Delta_AF7\", \"Delta_AF8\", \"Delta_TP10\",\n",
    "  \"Theta_TP9\", \"Theta_AF7\", \"Theta_AF8\", \"Theta_TP10\",\n",
    "  \"Alpha_TP9\", \"Alpha_AF7\", \"Alpha_AF8\", \"Alpha_TP10\",\n",
    "  \"Beta_TP9\",  \"Beta_AF7\",  \"Beta_AF8\",  \"Beta_TP10\",\n",
    "  \"Gamma_TP9\", \"Gamma_AF7\", \"Gamma_AF8\", \"Gamma_TP10\"\n",
    "]\n",
    "\n",
    "# Now only two states: Rest vs Music\n",
    "all_states = [\"Rest\", \"Music\"]\n",
    "n_states   = len(all_states)\n",
    "state2idx  = {s:i for i,s in enumerate(all_states)}\n",
    "idx2state  = {i:s for s,i in state2idx.items()}\n",
    "\n",
    "def load_subject(subj_dir):\n",
    "    \"\"\"Load one subject’s EEG + log, label Rest vs Music, and return (X, y_idx, times).\"\"\"\n",
    "    eeg_files = glob.glob(os.path.join(subj_dir, \"*mindMonitor*.csv\"))\n",
    "    log_files = glob.glob(os.path.join(subj_dir, \"*music_genre_experiment_log*.txt\"))\n",
    "    if not eeg_files or not log_files:\n",
    "        return None\n",
    "    # 1) Load and clean EEG\n",
    "    df = pd.read_csv(eeg_files[0])\n",
    "    if \"Elements\" in df.columns:\n",
    "        df = df[df[\"Elements\"].isna()]\n",
    "    df[\"Time\"] = pd.to_datetime(df[\"TimeStamp\"])\n",
    "    df.set_index(\"Time\", inplace=True)\n",
    "\n",
    "    # 2) Parse log and build two-state intervals\n",
    "    sync_df = pd.read_csv(log_files[0], sep=\"\\t\", header=None,\n",
    "                          names=[\"Event\",\"Time\"])\n",
    "    sync_df.dropna(inplace=True)\n",
    "    exp0 = float(sync_df.loc[\n",
    "        sync_df[\"Event\"].str.contains(\"Experiment Unix Start\"),\n",
    "        \"Time\"].iat[0])\n",
    "    t0 = exp0 + 2*3600\n",
    "    to_ts = lambda offs: pd.to_datetime(t0 + float(offs), unit=\"s\")\n",
    "\n",
    "    intervals = {\"Rest\": [], \"Music\": []}\n",
    "    # Baseline → Rest\n",
    "    b0 = sync_df.loc[sync_df[\"Event\"]==\"Baseline Start\", \"Time\"].iat[0]\n",
    "    b1 = sync_df.loc[sync_df[\"Event\"]==\"Baseline End\",   \"Time\"].iat[0]\n",
    "    intervals[\"Rest\"].append((to_ts(b0), to_ts(b1)))\n",
    "    # Music intervals (all Music Start → Music End)\n",
    "    for i,row in sync_df.iterrows():\n",
    "        ev, offs = row[\"Event\"], row[\"Time\"]\n",
    "        if ev.startswith(\"Music Start\"):\n",
    "            start = to_ts(offs)\n",
    "            end   = to_ts(sync_df.at[i+1, \"Time\"])\n",
    "            intervals[\"Music\"].append((start, end))\n",
    "        # Post Rest → also Rest\n",
    "        elif ev.startswith(\"Post Rest Start\"):\n",
    "            start = to_ts(offs)\n",
    "            end   = to_ts(sync_df.at[i+1, \"Time\"])\n",
    "            intervals[\"Rest\"].append((start, end))\n",
    "\n",
    "    # 3) Label each sample as Rest or Music\n",
    "    df[\"Condition\"] = \"Rest\"   # default everything to Rest\n",
    "    for start,end in intervals[\"Music\"]:\n",
    "        df.loc[(df.index>=start)&(df.index<=end), \"Condition\"] = \"Music\"\n",
    "\n",
    "    # Keep only labeled data\n",
    "    df = df[df[\"Condition\"].isin(all_states)]\n",
    "\n",
    "    # 4) Select features & build arrays\n",
    "    feature_cols = [c for c in selected_features if c in df.columns]\n",
    "    if not feature_cols:\n",
    "        feature_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    X = df[feature_cols].T.values       # shape = (n_features, n_timesteps)\n",
    "    y_idx = np.array([state2idx[c] for c in df[\"Condition\"].values])\n",
    "    times = df.index.to_numpy()\n",
    "    return X, y_idx, times\n",
    "\n",
    "# Load all subjects\n",
    "subject_dirs = sorted(glob.glob(os.path.join(root_dir, \"sub-*\")))\n",
    "subject_data = {}\n",
    "for sd in subject_dirs:\n",
    "    sid = os.path.basename(sd)\n",
    "    data = load_subject(sd)\n",
    "    if data is not None:\n",
    "        subject_data[sid] = data\n",
    "    else:\n",
    "        print(f\"{sid}: missing files, skipping\")\n",
    "\n",
    "# Leave-One-Subject-Out CV with two-state HMM\n",
    "for left_out in subject_data:\n",
    "    # 1) Accumulate stats from all OTHER subjects\n",
    "    init_counts  = np.zeros(n_states)\n",
    "    trans_counts = np.zeros((n_states,n_states))\n",
    "    emission_data = {s: [] for s in all_states}\n",
    "\n",
    "    for sid,(X,y_idx,_) in subject_data.items():\n",
    "        if sid == left_out: continue\n",
    "        # initial state\n",
    "        init_counts[y_idx[0]] += 1\n",
    "        # transitions\n",
    "        for a,b in zip(y_idx[:-1], y_idx[1:]):\n",
    "            trans_counts[a,b] += 1\n",
    "        # emissions\n",
    "        for st in all_states:\n",
    "            i = state2idx[st]\n",
    "            mask = (y_idx==i)\n",
    "            if mask.any():\n",
    "                emission_data[st].append(X[:,mask].T)\n",
    "\n",
    "    # Build π\n",
    "    pi = init_counts / init_counts.sum()\n",
    "    # Build A\n",
    "    A = (trans_counts.T / trans_counts.sum(axis=1)).T\n",
    "    for i in range(n_states):\n",
    "        if A[i].sum()==0:\n",
    "            A[i,i]=1.0\n",
    "\n",
    "    # Build Gaussian emissions\n",
    "    means, covars = [], []\n",
    "    reg = 1e-2\n",
    "    for st in all_states:\n",
    "        data = np.vstack(emission_data[st])\n",
    "        mu = data.mean(axis=0)\n",
    "        C  = np.cov(data, rowvar=False)\n",
    "        C  = 0.5*(C+C.T) + reg*np.eye(C.shape[1])\n",
    "        means.append(mu); covars.append(C)\n",
    "    means  = np.vstack(means)\n",
    "    covars = np.stack(covars)\n",
    "\n",
    "    # Construct HMM\n",
    "    model = hmm.GaussianHMM(n_components=n_states,\n",
    "                            covariance_type=\"full\",\n",
    "                            init_params=\"\")\n",
    "    model.startprob_ = pi\n",
    "    model.transmat_  = A\n",
    "    model.means_     = means\n",
    "    model.covars_    = covars\n",
    "\n",
    "    # 2) Test on left-out subject\n",
    "    X_test, y_test_idx, times_test = subject_data[left_out]\n",
    "    _, y_pred_idx = model.decode(X_test.T, algorithm=\"viterbi\")\n",
    "    acc = (y_pred_idx==y_test_idx).mean()\n",
    "    print(f\"{left_out}: LOO binary accuracy = {acc*100:.2f}%\")\n",
    "\n",
    "    # 3) Plot True vs Predicted\n",
    "    fig, ax = plt.subplots(figsize=(10,3))\n",
    "    ax.step(times_test, y_test_idx,    where=\"post\",\n",
    "            linestyle=\"--\", label=\"True\",   lw=1)\n",
    "    ax.step(times_test, y_pred_idx,    where=\"post\",\n",
    "            linestyle=\"-\",  label=\"Predicted\", lw=1.5)\n",
    "    ax.set_yticks([0,1])\n",
    "    ax.set_yticklabels(all_states)\n",
    "    ax.set_title(f\"{left_out}: True vs Predicted (acc {acc*100:.1f}%)\")\n",
    "    ax.set_xlabel(\"Time\"); ax.grid(axis=\"x\", linestyle=\"--\", alpha=0.3)\n",
    "    ax.legend(loc=\"upper right\")\n",
    "    fig.autofmt_xdate()\n",
    "    plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from hmmlearn import hmm\n",
    "\n",
    "# === User settings ===\n",
    "root_dir = \"/Users/kyrusmama/Downloads/ref_prep_synchr_MAT_jiahui/Data_MGR\"\n",
    "selected_features = [\n",
    "  \"Delta_TP9\", \"Delta_AF7\", \"Delta_AF8\", \"Delta_TP10\",\n",
    "  \"Theta_TP9\", \"Theta_AF7\", \"Theta_AF8\", \"Theta_TP10\",\n",
    "  \"Alpha_TP9\", \"Alpha_AF7\", \"Alpha_AF8\", \"Alpha_TP10\",\n",
    "  \"Beta_TP9\",  \"Beta_AF7\",  \"Beta_AF8\",  \"Beta_TP10\",\n",
    "  \"Gamma_TP9\", \"Gamma_AF7\", \"Gamma_AF8\", \"Gamma_TP10\"\n",
    "]\n",
    "\n",
    "# The four states\n",
    "all_states = [\"Rest\", \"Jazz\", \"Classical\", \"Electronic\"]\n",
    "n_states   = len(all_states)\n",
    "state2idx  = {s:i for i,s in enumerate(all_states)}\n",
    "idx2state  = {i:s for s,i in state2idx.items()}\n",
    "\n",
    "def load_subject(subj_dir):\n",
    "    subj_id = os.path.basename(subj_dir)\n",
    "    eeg_files = glob.glob(os.path.join(subj_dir, \"*mindMonitor*.csv\"))\n",
    "    log_files = glob.glob(os.path.join(subj_dir, \"*music_genre_experiment_log*.txt\"))\n",
    "    if not eeg_files or not log_files:\n",
    "        return None\n",
    "\n",
    "    # --- 1) Load & clean EEG ---\n",
    "    df = pd.read_csv(eeg_files[0])\n",
    "    if \"Elements\" in df.columns:\n",
    "        df = df[df[\"Elements\"].isna()]\n",
    "    df[\"Time\"] = pd.to_datetime(df[\"TimeStamp\"])\n",
    "    df.set_index(\"Time\", inplace=True)\n",
    "\n",
    "    # --- 2) Detect & replace dead channels per band ---\n",
    "    bands    = [\"Delta\",\"Theta\",\"Alpha\",\"Beta\",\"Gamma\"]\n",
    "    channels = [\"TP9\",\"AF7\",\"AF8\",\"TP10\"]\n",
    "    for band in bands:\n",
    "        cols = [f\"{band}_{ch}\" for ch in channels if f\"{band}_{ch}\" in df.columns]\n",
    "        if len(cols) < 2:\n",
    "            continue\n",
    "        stds = df[cols].std()\n",
    "        median_std = stds.median()\n",
    "        dead = stds[stds < 0.1 * median_std].index.tolist()\n",
    "        if not dead:\n",
    "            continue\n",
    "        alive = [c for c in cols if c not in dead]\n",
    "        if not alive:\n",
    "            print(f\"{subj_id}: warning – all {band} channels dead, skipping replacement\")\n",
    "            continue\n",
    "        for dead_col in dead:\n",
    "            df[dead_col] = df[alive].mean(axis=1)\n",
    "            print(f\"{subj_id}: warning – dead channel {dead_col}, replaced with mean of {alive}\")\n",
    "\n",
    "    # --- 3) Normalize each feature (z-score) ---\n",
    "    for col in selected_features:\n",
    "        if col in df.columns:\n",
    "            mu, sigma = df[col].mean(), df[col].std()\n",
    "            df[col] = (df[col] - mu) / (sigma if sigma>0 else 1.0)\n",
    "\n",
    "    # --- 4) Parse log & build intervals ---\n",
    "    sync_df = pd.read_csv(log_files[0], sep=\"\\t\", header=None,\n",
    "                          names=[\"Event\",\"Time\"])\n",
    "    sync_df.dropna(inplace=True)\n",
    "    exp0 = float(sync_df.loc[\n",
    "        sync_df[\"Event\"].str.contains(\"Experiment Unix Start\"), \"Time\"\n",
    "    ].iat[0])\n",
    "    t0 = exp0 + 2*3600\n",
    "    to_ts = lambda offs: pd.to_datetime(t0 + float(offs), unit=\"s\")\n",
    "\n",
    "    intervals = {s: [] for s in all_states}\n",
    "    # Baseline → Rest\n",
    "    b0 = sync_df.loc[sync_df[\"Event\"]==\"Baseline Start\", \"Time\"].iat[0]\n",
    "    b1 = sync_df.loc[sync_df[\"Event\"]==\"Baseline End\",   \"Time\"].iat[0]\n",
    "    intervals[\"Rest\"].append((to_ts(b0), to_ts(b1)))\n",
    "    # Music & Post‐Rest\n",
    "    for i,row in sync_df.iterrows():\n",
    "        ev, offs = row[\"Event\"], row[\"Time\"]\n",
    "        if ev.startswith(\"Music Start\"):\n",
    "            genre = ev.split(\" - \")[2]\n",
    "            start = to_ts(offs)\n",
    "            end   = to_ts(sync_df.at[i+1, \"Time\"])\n",
    "            intervals[genre].append((start,end))\n",
    "        elif ev.startswith(\"Post Rest Start\"):\n",
    "            start = to_ts(offs)\n",
    "            end   = to_ts(sync_df.at[i+1, \"Time\"])\n",
    "            intervals[\"Rest\"].append((start,end))\n",
    "\n",
    "    # --- 5) Label each sample ---\n",
    "    df[\"Condition\"] = \"Unlabeled\"\n",
    "    for cond, spans in intervals.items():\n",
    "        for start,end in spans:\n",
    "            df.loc[(df.index>=start)&(df.index<=end),\"Condition\"] = cond\n",
    "    df = df[df[\"Condition\"]!=\"Unlabeled\"]\n",
    "\n",
    "    # --- 6) Build arrays ---\n",
    "    feature_cols = [c for c in selected_features if c in df.columns]\n",
    "    if not feature_cols:\n",
    "        feature_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    X      = df[feature_cols].T.values\n",
    "    y_idx  = np.array([state2idx[c] for c in df[\"Condition\"].values])\n",
    "    times  = df.index.to_numpy()\n",
    "    return X, y_idx, times\n",
    "\n",
    "# === Load & preprocess all subjects ===\n",
    "subject_dirs = sorted(glob.glob(os.path.join(root_dir, \"sub-*\")))\n",
    "subject_data = {}\n",
    "for sd in subject_dirs:\n",
    "    sid = os.path.basename(sd)\n",
    "    data = load_subject(sd)\n",
    "    if data:\n",
    "        subject_data[sid] = data\n",
    "    else:\n",
    "        print(f\"{sid}: missing data/log → skipping\")\n",
    "\n",
    "# === Leave-One-Subject-Out Cross‐Validation ===\n",
    "for left_out in subject_data:\n",
    "    # Train on all but left_out\n",
    "    init_counts   = np.zeros(n_states)\n",
    "    trans_counts  = np.zeros((n_states,n_states))\n",
    "    emission_data = {s: [] for s in all_states}\n",
    "\n",
    "    for sid, (X, y_idx, _) in subject_data.items():\n",
    "        if sid == left_out: continue\n",
    "        init_counts[y_idx[0]] += 1\n",
    "        for a,b in zip(y_idx[:-1], y_idx[1:]):\n",
    "            trans_counts[a,b] += 1\n",
    "        for state in all_states:\n",
    "            i = state2idx[state]\n",
    "            mask = (y_idx==i)\n",
    "            if mask.any():\n",
    "                emission_data[state].append(X[:,mask].T)\n",
    "\n",
    "    # π\n",
    "    pi = init_counts / init_counts.sum()\n",
    "    # A\n",
    "    A = (trans_counts.T / trans_counts.sum(axis=1)).T\n",
    "    for i in range(n_states):\n",
    "        if A[i].sum()==0: A[i,i]=1.0\n",
    "\n",
    "    # emissions\n",
    "    means, covars = [], []\n",
    "    reg = 1e-2\n",
    "    for state in all_states:\n",
    "        data = np.vstack(emission_data[state])\n",
    "        mu = data.mean(axis=0)\n",
    "        C  = np.cov(data, rowvar=False)\n",
    "        C  = 0.5*(C + C.T) + reg*np.eye(C.shape[1])\n",
    "        means.append(mu); covars.append(C)\n",
    "    means  = np.vstack(means)\n",
    "    covars = np.stack(covars)\n",
    "\n",
    "    # build & decode HMM\n",
    "    model = hmm.GaussianHMM(n_components=n_states,\n",
    "                            covariance_type=\"full\",\n",
    "                            init_params=\"\")\n",
    "    model.startprob_, model.transmat_, model.means_, model.covars_ = pi, A, means, covars\n",
    "\n",
    "    X_test, y_test_idx, times_test = subject_data[left_out]\n",
    "    _, y_pred_idx = model.decode(X_test.T, algorithm=\"viterbi\")\n",
    "    acc = (y_pred_idx==y_test_idx).mean()\n",
    "    print(f\"{left_out}: LOO accuracy = {acc*100:.2f}%\")\n",
    "\n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(10,3))\n",
    "    ax.step(times_test, y_test_idx,    where=\"post\", linestyle=\"--\", label=\"True\",   lw=1)\n",
    "    ax.step(times_test, y_pred_idx,    where=\"post\", linestyle=\"-\",  label=\"Predicted\", lw=1.5)\n",
    "    ax.set_yticks(range(n_states)); ax.set_yticklabels(all_states)\n",
    "    ax.set_title(f\"{left_out}: True vs Predicted (LOO acc {acc*100:.1f}%)\")\n",
    "    ax.set_xlabel(\"Time\"); ax.grid(axis=\"x\", linestyle=\"--\", alpha=0.3)\n",
    "    ax.legend(loc=\"upper right\", fontsize=\"small\")\n",
    "    fig.autofmt_xdate(); plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "drop subject 3, 16, 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from hmmlearn import hmm\n",
    "\n",
    "# === User settings ===\n",
    "root_dir = \"/Users/kyrusmama/Downloads/ref_prep_synchr_MAT_jiahui/Data_MGR\"\n",
    "selected_features = [\n",
    "  \"Delta_TP9\", \"Delta_AF7\", \"Delta_AF8\", \"Delta_TP10\",\n",
    "  \"Theta_TP9\", \"Theta_AF7\", \"Theta_AF8\", \"Theta_TP10\",\n",
    "  \"Alpha_TP9\", \"Alpha_AF7\", \"Alpha_AF8\", \"Alpha_TP10\",\n",
    "  \"Beta_TP9\",  \"Beta_AF7\",  \"Beta_AF8\",  \"Beta_TP10\",\n",
    "  \"Gamma_TP9\", \"Gamma_AF7\", \"Gamma_AF8\", \"Gamma_TP10\"\n",
    "]\n",
    "\n",
    "# Now only two states: Rest vs Music\n",
    "all_states = [\"Rest\", \"Music\"]\n",
    "state2idx  = {s:i for i,s in enumerate(all_states)}\n",
    "idx2state  = {i:s for s,i in state2idx.items()}\n",
    "n_states   = len(all_states)\n",
    "\n",
    "def load_subject(subj_dir):\n",
    "    subj_id = os.path.basename(subj_dir)\n",
    "    # find files\n",
    "    eeg_files = glob.glob(os.path.join(subj_dir, \"*mindMonitor*.csv\"))\n",
    "    log_files = glob.glob(os.path.join(subj_dir, \"*music_genre_experiment_log*.txt\"))\n",
    "    if not eeg_files or not log_files:\n",
    "        return None\n",
    "    # 1) Load & clean EEG\n",
    "    df = pd.read_csv(eeg_files[0])\n",
    "    if \"Elements\" in df.columns:\n",
    "        df = df[df[\"Elements\"].isna()]\n",
    "    df[\"Time\"] = pd.to_datetime(df[\"TimeStamp\"])\n",
    "    df.set_index(\"Time\", inplace=True)\n",
    "\n",
    "    # 2) Dead‐channel detection & replacement\n",
    "    bands    = [\"Delta\",\"Theta\",\"Alpha\",\"Beta\",\"Gamma\"]\n",
    "    channels = [\"TP9\",\"AF7\",\"AF8\",\"TP10\"]\n",
    "    for band in bands:\n",
    "        cols = [f\"{band}_{ch}\" for ch in channels if f\"{band}_{ch}\" in df.columns]\n",
    "        if len(cols) < 2:\n",
    "            continue\n",
    "        stds = df[cols].std()\n",
    "        median_std = stds.median()\n",
    "        dead = stds[stds < 0.1*median_std].index.tolist()\n",
    "        alive = [c for c in cols if c not in dead]\n",
    "        for dead_col in dead:\n",
    "            if alive:\n",
    "                df[dead_col] = df[alive].mean(axis=1)\n",
    "                print(f\"{subj_id}: warning – dead channel {dead_col}, replaced with mean of {alive}\")\n",
    "            else:\n",
    "                print(f\"{subj_id}: warning – all {band} channels dead, cannot replace\")\n",
    "\n",
    "    # 3) Feature‐wise z‐score normalization\n",
    "    for col in selected_features:\n",
    "        if col in df.columns:\n",
    "            mu, sigma = df[col].mean(), df[col].std()\n",
    "            df[col] = (df[col] - mu) / (sigma if sigma>0 else 1.0)\n",
    "\n",
    "    # 4) Parse log & build intervals\n",
    "    sync_df = pd.read_csv(log_files[0], sep=\"\\t\", header=None, names=[\"Event\",\"Time\"])\n",
    "    sync_df.dropna(inplace=True)\n",
    "    exp0 = float(sync_df.loc[sync_df[\"Event\"].str.contains(\"Experiment Unix Start\"), \"Time\"].iat[0])\n",
    "    t0   = exp0 + 2*3600\n",
    "    to_ts = lambda offs: pd.to_datetime(t0 + float(offs), unit=\"s\")\n",
    "\n",
    "    intervals = {\"Rest\": [], \"Music\": []}\n",
    "    # Baseline → Rest\n",
    "    b0 = sync_df.loc[sync_df[\"Event\"]==\"Baseline Start\", \"Time\"].iat[0]\n",
    "    b1 = sync_df.loc[sync_df[\"Event\"]==\"Baseline End\",   \"Time\"].iat[0]\n",
    "    intervals[\"Rest\"].append((to_ts(b0), to_ts(b1)))\n",
    "    # Music trials → Music; Post‐Rest → Rest\n",
    "    for i,row in sync_df.iterrows():\n",
    "        ev, offs = row[\"Event\"], row[\"Time\"]\n",
    "        if ev.startswith(\"Music Start\"):\n",
    "            start = to_ts(offs)\n",
    "            end   = to_ts(sync_df.at[i+1, \"Time\"])\n",
    "            intervals[\"Music\"].append((start, end))\n",
    "        elif ev.startswith(\"Post Rest Start\"):\n",
    "            start = to_ts(offs)\n",
    "            end   = to_ts(sync_df.at[i+1, \"Time\"])\n",
    "            intervals[\"Rest\"].append((start, end))\n",
    "\n",
    "    # 5) Label samples as Rest vs Music\n",
    "    df[\"Condition\"] = \"Unlabeled\"\n",
    "    for cond, spans in intervals.items():\n",
    "        for start,end in spans:\n",
    "            df.loc[(df.index>=start)&(df.index<=end), \"Condition\"] = cond\n",
    "    df = df[df[\"Condition\"]!=\"Unlabeled\"]\n",
    "\n",
    "    # 6) Build arrays\n",
    "    feature_cols = [c for c in selected_features if c in df.columns]\n",
    "    if not feature_cols:\n",
    "        feature_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    X     = df[feature_cols].T.values\n",
    "    y_idx = np.array([state2idx[c] for c in df[\"Condition\"].values])\n",
    "    times = df.index.to_numpy()\n",
    "    return X, y_idx, times\n",
    "\n",
    "# Load all subjects\n",
    "subject_dirs = sorted(glob.glob(os.path.join(root_dir, \"sub-*\")))\n",
    "subject_data = {}\n",
    "for sd in subject_dirs:\n",
    "    sid = os.path.basename(sd)\n",
    "    data = load_subject(sd)\n",
    "    if data:\n",
    "        subject_data[sid] = data\n",
    "    else:\n",
    "        print(f\"{sid}: missing or incomplete, skipping\")\n",
    "\n",
    "# Leave‐one‐subject‐out CV with binary states\n",
    "for left_out in subject_data:\n",
    "    # Accumulators\n",
    "    init_counts   = np.zeros(n_states)\n",
    "    trans_counts  = np.zeros((n_states,n_states))\n",
    "    emission_data = {\"Rest\": [], \"Music\": []}\n",
    "\n",
    "    # Train on all but left_out\n",
    "    for sid, (X, y_idx, _) in subject_data.items():\n",
    "        if sid == left_out: continue\n",
    "        init_counts[y_idx[0]] += 1\n",
    "        for a,b in zip(y_idx[:-1], y_idx[1:]):\n",
    "            trans_counts[a,b] += 1\n",
    "        for state in all_states:\n",
    "            i = state2idx[state]\n",
    "            mask = (y_idx==i)\n",
    "            if mask.any():\n",
    "                emission_data[state].append(X[:,mask].T)\n",
    "\n",
    "    # Compute π, A\n",
    "    pi = init_counts / init_counts.sum()\n",
    "    A  = (trans_counts.T / trans_counts.sum(axis=1)).T\n",
    "    for i in range(n_states):\n",
    "        if A[i].sum()==0: A[i,i]=1.0\n",
    "\n",
    "    # Fit Gaussians\n",
    "    means, covars = [], []\n",
    "    reg = 1e-2\n",
    "    for state in all_states:\n",
    "        data = np.vstack(emission_data[state])\n",
    "        mu = data.mean(axis=0)\n",
    "        C  = np.cov(data, rowvar=False)\n",
    "        C  = 0.5*(C + C.T) + reg*np.eye(C.shape[1])\n",
    "        means.append(mu); covars.append(C)\n",
    "    means  = np.vstack(means)\n",
    "    covars = np.stack(covars)\n",
    "\n",
    "    # Build HMM\n",
    "    model = hmm.GaussianHMM(n_components=n_states,\n",
    "                            covariance_type=\"full\",\n",
    "                            init_params=\"\")\n",
    "    model.startprob_, model.transmat_, model.means_, model.covars_ = pi, A, means, covars\n",
    "\n",
    "    # Test on left‐out\n",
    "    X_test, y_test_idx, times_test = subject_data[left_out]\n",
    "    _, y_pred_idx = model.decode(X_test.T, algorithm=\"viterbi\")\n",
    "    acc = (y_pred_idx==y_test_idx).mean()\n",
    "    print(f\"{left_out}: LOO Music-vs-Rest acc = {acc*100:.2f}%\")\n",
    "\n",
    "    # Plot true vs predicted\n",
    "    fig, ax = plt.subplots(figsize=(10,3))\n",
    "    ax.step(times_test, y_test_idx,    where=\"post\",\n",
    "            linestyle=\"--\", label=\"True\",   lw=1)\n",
    "    ax.step(times_test, y_pred_idx,    where=\"post\",\n",
    "            linestyle=\"-\",  label=\"Predicted\", lw=1.5)\n",
    "    ax.set_yticks([0,1]); ax.set_yticklabels(all_states)\n",
    "    ax.set_title(f\"{left_out}: Music vs Rest (LOO acc {acc*100:.1f}%)\")\n",
    "    ax.set_xlabel(\"Time\"); ax.grid(axis=\"x\", linestyle=\"--\", alpha=0.3)\n",
    "    ax.legend(loc=\"upper right\", fontsize=\"small\")\n",
    "    fig.autofmt_xdate(); plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# baseline correction for each band (single subject), corrected by averaged band 1 s before each task onset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Load EEG data\n",
    "#eeg_file = \"C:/Users/Jiahui An/Geo/CC_project/Data/sub-10/mindMonitor_2025-04-30--14-23-31_10.csv\"\n",
    "df = pd.read_csv(eeg_file)\n",
    "df[\"Time\"] = pd.to_datetime(df[\"TimeStamp\"])\n",
    "df.set_index(\"Time\", inplace=True)\n",
    "\n",
    "# Load log file\n",
    "#log_file = \"C:/Users/Jiahui An/Geo/CC_project/Data/sub-10/eeg_experiment_timeline_log_10.txt\"\n",
    "sync_df = pd.read_csv(log_file, sep='\\t', header=None, names=[\"Event\", \"Time\"]).dropna()\n",
    "\n",
    "# Extract start time\n",
    "start_unix = float(sync_df[sync_df[\"Event\"].str.contains(\"Experiment Unix Start\")][\"Time\"].values[0]) + 2*3600\n",
    "\n",
    "# Define bands and channels\n",
    "bands = [\"Delta\", \"Theta\", \"Alpha\", \"Beta\"]\n",
    "channels = [\"TP9\", \"AF7\", \"AF8\", \"TP10\"]\n",
    "band_cols = [f\"{band}_{ch}\" for band in bands for ch in channels if f\"{band}_{ch}\" in df.columns]\n",
    "\n",
    "# Compute average band power per band\n",
    "df_band_avg = pd.DataFrame(index=df.index)\n",
    "for band in bands:\n",
    "    cols = [f\"{band}_{ch}\" for ch in channels if f\"{band}_{ch}\" in df.columns]\n",
    "    df_band_avg[band] = df[cols].mean(axis=1)\n",
    "\n",
    "# === Segment labeling and local baseline correction ===\n",
    "segments = []\n",
    "baseline_window_sec = 1  # 1 second before segment start\n",
    "\n",
    "for i in range(len(sync_df) - 1):\n",
    "    event = sync_df.iloc[i][\"Event\"]\n",
    "    t0 = pd.to_datetime(start_unix + sync_df.iloc[i][\"Time\"], unit=\"s\")\n",
    "    t1 = pd.to_datetime(start_unix + sync_df.iloc[i + 1][\"Time\"], unit=\"s\")\n",
    "\n",
    "    if any(x in event for x in [\"Task Start\", \"Rest Start\", \"Music Start\", \"Post-Rest Start\"]):\n",
    "        label = (\n",
    "            \"Task\" if \"Task\" in event else\n",
    "            \"Rest\" if \"Rest Start\" in event and \"Post\" not in event else\n",
    "            \"Post-Rest\" if \"Post-Rest\" in event else\n",
    "            \"Music\"\n",
    "        )\n",
    "\n",
    "        # Define segment and local baseline window\n",
    "        segment_data = df_band_avg.loc[(df_band_avg.index >= t0) & (df_band_avg.index < t1)].copy()\n",
    "        baseline_data = df_band_avg.loc[(df_band_avg.index >= t0 - pd.Timedelta(seconds=baseline_window_sec)) & (df_band_avg.index < t0)]\n",
    "\n",
    "        # Compute local baseline mean per band\n",
    "        baseline_mean = baseline_data.mean()\n",
    "\n",
    "        # Subtract baseline\n",
    "        corrected = segment_data[bands] - baseline_mean\n",
    "\n",
    "        # Add features\n",
    "        corrected[\"Engagement\"] = corrected[\"Beta\"] / (corrected[\"Alpha\"] + corrected[\"Theta\"] + 1e-6)\n",
    "        corrected[\"Theta/Alpha\"] = corrected[\"Theta\"] / (corrected[\"Alpha\"] + 1e-6)\n",
    "        corrected[\"Alpha/Theta\"] = corrected[\"Alpha\"] / (corrected[\"Theta\"] + 1e-6)\n",
    "\n",
    "        # Add condition label\n",
    "        corrected[\"Condition\"] = label\n",
    "\n",
    "        segments.append(corrected)\n",
    "\n",
    "# Combine all labeled segments\n",
    "df_all = pd.concat(segments)\n",
    "\n",
    "# Optional plot\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(\n",
    "    data=df_all.melt(id_vars=\"Condition\", value_vars=[\"Engagement\", \"Theta/Alpha\", \"Alpha/Theta\"]),\n",
    "    x=\"variable\", y=\"value\", hue=\"Condition\"\n",
    ")\n",
    "plt.title(\"Cognitive Load Indices (Locally Baseline-Corrected)\")\n",
    "plt.ylabel(\"Normalized Value\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nCondition counts:\")\n",
    "print(df_all[\"Condition\"].value_counts())\n",
    "# Label baseline segment\n",
    "baseline_start = pd.to_datetime(start_unix + sync_df[sync_df[\"Event\"] == \"Baseline Start\"][\"Time\"].values[0], unit=\"s\")\n",
    "baseline_end = pd.to_datetime(start_unix + sync_df[sync_df[\"Event\"] == \"Baseline End\"][\"Time\"].values[0], unit=\"s\")\n",
    "baseline_data = df_band_avg.loc[(df_band_avg.index >= baseline_start) & (df_band_avg.index < baseline_end)].copy()\n",
    "\n",
    "# Baseline correction (from 1s before baseline start)\n",
    "baseline_window = df_band_avg.loc[(df_band_avg.index >= baseline_start - pd.Timedelta(seconds=1)) & (df_band_avg.index < baseline_start)]\n",
    "baseline_mean = baseline_window.mean()\n",
    "corrected_baseline = baseline_data[bands] - baseline_mean\n",
    "\n",
    "corrected_baseline[\"Engagement\"] = corrected_baseline[\"Beta\"] / (corrected_baseline[\"Alpha\"] + corrected_baseline[\"Theta\"] + 1e-6)\n",
    "corrected_baseline[\"Theta/Alpha\"] = corrected_baseline[\"Theta\"] / (corrected_baseline[\"Alpha\"] + 1e-6)\n",
    "corrected_baseline[\"Alpha/Theta\"] = corrected_baseline[\"Alpha\"] / (corrected_baseline[\"Theta\"] + 1e-6)\n",
    "corrected_baseline[\"Condition\"] = \"Baseline\"\n",
    "\n",
    "segments.append(corrected_baseline)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# statistical comparison between each condition for each features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "\n",
    "# Features to compare\n",
    "features = [\"Engagement\", \"Theta/Alpha\", \"Alpha/Theta\"]\n",
    "\n",
    "# Prepare groups\n",
    "group_task = df_all[df_all[\"Condition\"] == \"Task\"]\n",
    "group_rest = df_all[df_all[\"Condition\"] == \"Rest\"]\n",
    "group_music = df_all[df_all[\"Condition\"] == \"Music\"]\n",
    "\n",
    "print(\"=== Statistical Comparison ===\\n\")\n",
    "\n",
    "# Compare Task vs. Rest\n",
    "print(\"Task vs. Rest\")\n",
    "for feat in features:\n",
    "    t, p = ttest_ind(group_task[feat], group_rest[feat], equal_var=False, nan_policy=\"omit\")\n",
    "    print(f\"{feat:<12}: t = {t:8.4f}, p = {p:8.4f}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# Compare Music vs. Rest\n",
    "print(\"Music vs. Rest\")\n",
    "for feat in features:\n",
    "    t, p = ttest_ind(group_music[feat], group_rest[feat], equal_var=False, nan_policy=\"omit\")\n",
    "    print(f\"{feat:<12}: t = {t:8.4f}, p = {p:8.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## using absolute values of the band power, baseline corrected (- averaged rest condition), plus statistical analysis\n",
    "\n",
    "to do: consider transient effect, explore the optimal task duration and rest duration for averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we took the absolute values\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load pre-processed EEG band power data with condition labels\n",
    "file_path = \"/Users/kyrusmama/Downloads/ref_prep_synchr_MAT_jiahui/Data_all/sub-10/features.csv\"\n",
    "df = pd.read_csv(file_path, parse_dates=[\"Time\"])\n",
    "df.set_index(\"Time\", inplace=True)\n",
    "\n",
    "# Define features based on whole-brain average bands (per row)\n",
    "eps = 1e-6\n",
    "df[\"Engagement\"] = df[\"Beta\"] / (df[\"Alpha\"] + df[\"Theta\"] + eps)\n",
    "df[\"Theta/Alpha\"] = df[\"Theta\"] / (df[\"Alpha\"] + eps)\n",
    "df[\"Alpha/Theta\"] = df[\"Alpha\"] / (df[\"Theta\"] + eps)\n",
    "\n",
    "# Aggregate per condition\n",
    "features = [\"Alpha\", \"Theta\",\"Engagement\", \"Theta/Alpha\", \"Alpha/Theta\"]\n",
    "condition_avg = df.groupby(\"Condition\")[features].mean()\n",
    "\n",
    "# Baseline correction\n",
    "baseline_mean = condition_avg.loc[\"Rest\"]\n",
    "condition_corrected = condition_avg - baseline_mean\n",
    "\n",
    "# Normalize (absolute values)\n",
    "condition_corrected_normalized = condition_corrected.abs()\n",
    "\n",
    "# Plot the normalized values\n",
    "condition_corrected_normalized.T.plot(kind=\"bar\", figsize=(10, 5), colormap=\"Set2\")\n",
    "plt.title(\"Baseline-Corrected Cognitive Load Indices (Whole-Brain Band Averages)\")\n",
    "plt.ylabel(\"Normalized Value\")\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.xticks(rotation=0)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "# Drop baseline and keep only Task and Rest\n",
    "df = df[df[\"Condition\"].isin([\"Task\", \"Rest\"])].copy()\n",
    "\n",
    "# Recompute features after baseline correction per row\n",
    "df[\"Engagement\"] = df[\"Beta\"] / (df[\"Alpha\"] + df[\"Theta\"] + eps)\n",
    "df[\"Theta/Alpha\"] = df[\"Theta\"] / (df[\"Alpha\"] + eps)\n",
    "df[\"Alpha/Theta\"] = df[\"Alpha\"] / (df[\"Theta\"] + eps)\n",
    "\n",
    "# Apply baseline correction: subtract per-feature baseline mean\n",
    "for f in features:\n",
    "    df[f] = df[f] - baseline_mean[f]\n",
    "\n",
    "# Take absolute values (normalize)\n",
    "df[features] = df[features].abs()\n",
    "\n",
    "# Run t-tests\n",
    "results = []\n",
    "for f in features:\n",
    "    task_vals = df[df[\"Condition\"] == \"Task\"][f].dropna()\n",
    "    rest_vals = df[df[\"Condition\"] == \"Rest\"][f].dropna()\n",
    "    t_stat, p_val = ttest_ind(task_vals, rest_vals, equal_var=False)\n",
    "    results.append({\"Feature\": f, \"t-value\": t_stat, \"p-value\": p_val})\n",
    "\n",
    "# Display as DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"=== Task vs. Rest Statistical Comparison ===\")\n",
    "print(results_df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generating labeled data for each participant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# === Setup ===\n",
    "base_path = \"/Users/kyrusmama/Downloads/ref_prep_synchr_MAT_jiahui/Data_all\"\n",
    "bands = [\"Delta\", \"Theta\", \"Alpha\", \"Beta\"]\n",
    "channels = [\"TP9\", \"AF7\", \"AF8\", \"TP10\"]\n",
    "\n",
    "# === Loop through all subject folders ===\n",
    "for subfolder in sorted(os.listdir(base_path)):\n",
    "    sub_path = os.path.join(base_path, subfolder)\n",
    "    if not os.path.isdir(sub_path):\n",
    "        continue\n",
    "\n",
    "    print(f\"\\nProcessing {subfolder}...\")\n",
    "\n",
    "    # Find EEG and log file\n",
    "    eeg_file = next((f for f in os.listdir(sub_path) if f.endswith(\".csv\")), None)\n",
    "    log_file = next((f for f in os.listdir(sub_path) if f.endswith(\".txt\")), None)\n",
    "    if eeg_file is None or log_file is None:\n",
    "        print(f\"Missing EEG or log file in {subfolder}\")\n",
    "        continue\n",
    "\n",
    "    # === Load EEG ===\n",
    "    df = pd.read_csv(os.path.join(sub_path, eeg_file))\n",
    "    df[\"Time\"] = pd.to_datetime(df[\"TimeStamp\"])\n",
    "    df.set_index(\"Time\", inplace=True)\n",
    "\n",
    "    # === Load Log ===\n",
    "    log_df = pd.read_csv(os.path.join(sub_path, log_file), sep=\"\\t\", header=None, names=[\"Event\", \"Time\"]).dropna()\n",
    "\n",
    "    # === Extract and correct experiment start time ===\n",
    "    try:\n",
    "        start_unix = float(log_df[log_df[\"Event\"].str.contains(\"Experiment Unix Start\")][\"Time\"].values[0]) + 2 * 3600\n",
    "    except Exception as e:\n",
    "        print(f\"Could not extract start time in {subfolder}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # === Compute average band power per band ===\n",
    "    df_band_avg = pd.DataFrame(index=df.index)\n",
    "    for band in bands:\n",
    "        cols = [f\"{band}_{ch}\" for ch in channels if f\"{band}_{ch}\" in df.columns]\n",
    "        df_band_avg[band] = df[cols].mean(axis=1)\n",
    "\n",
    "    # === Segment labeling and local baseline correction ===\n",
    "    segments = []\n",
    "    baseline_window_sec = 1  # 1 second before segment start\n",
    "\n",
    "    for i in range(len(log_df) - 1):\n",
    "        event = log_df.iloc[i][\"Event\"]\n",
    "        t0 = pd.to_datetime(start_unix + log_df.iloc[i][\"Time\"], unit=\"s\")\n",
    "        t1 = pd.to_datetime(start_unix + log_df.iloc[i + 1][\"Time\"], unit=\"s\")\n",
    "\n",
    "        if any(x in event for x in [\"Task Start\", \"Rest Start\", \"Music Start\", \"Post-Rest Start\"]):\n",
    "            label = (\n",
    "                \"Task\" if \"Task\" in event else\n",
    "                \"Rest\" if \"Rest Start\" in event and \"Post\" not in event else\n",
    "                \"Post-Rest\" if \"Post-Rest\" in event else\n",
    "                \"Music\"\n",
    "            )\n",
    "\n",
    "            segment_data = df_band_avg.loc[(df_band_avg.index >= t0) & (df_band_avg.index < t1)].copy()\n",
    "            baseline_data = df_band_avg.loc[(df_band_avg.index >= t0 - pd.Timedelta(seconds=baseline_window_sec)) & (df_band_avg.index < t0)]\n",
    "\n",
    "            if baseline_data.empty or segment_data.empty:\n",
    "                continue\n",
    "\n",
    "            baseline_mean = baseline_data.mean()\n",
    "            corrected = segment_data[bands] - baseline_mean\n",
    "\n",
    "            corrected[\"Engagement\"] = corrected[\"Beta\"] / (corrected[\"Alpha\"] + corrected[\"Theta\"] + 1e-6)\n",
    "            corrected[\"Theta/Alpha\"] = corrected[\"Theta\"] / (corrected[\"Alpha\"] + 1e-6)\n",
    "            corrected[\"Alpha/Theta\"] = corrected[\"Alpha\"] / (corrected[\"Theta\"] + 1e-6)\n",
    "            corrected[\"Condition\"] = label\n",
    "\n",
    "            segments.append(corrected)\n",
    "\n",
    "    if not segments:\n",
    "        print(f\"No valid segments found in {subfolder}.\")\n",
    "        continue\n",
    "\n",
    "    df_all = pd.concat(segments)\n",
    "    out_csv = os.path.join(sub_path, \"features.csv\")\n",
    "    df_all.to_csv(out_csv)\n",
    "    print(f\"Saved features to: {out_csv}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## group level statistical analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "# === Setup ===\n",
    "base_path = \"/Users/kyrusmama/Downloads/ref_prep_synchr_MAT_jiahui/Data_all\"\n",
    "features_to_compare = [\"Engagement\", \"Theta/Alpha\", \"Alpha/Theta\", \"Theta\", \"Alpha\"]\n",
    "all_data = []\n",
    "\n",
    "# === Load all features.csv files ===\n",
    "for subfolder in sorted(os.listdir(base_path)):\n",
    "    file_path = os.path.join(base_path, subfolder, \"features.csv\")\n",
    "    if os.path.exists(file_path):\n",
    "        df = pd.read_csv(file_path, parse_dates=[\"Time\"])\n",
    "        df[\"Participant\"] = subfolder\n",
    "        all_data.append(df)\n",
    "\n",
    "# Combine all participants\n",
    "df_all = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "# === Summary counts ===\n",
    "print(\"Condition counts:\")\n",
    "print(df_all[\"Condition\"].value_counts())\n",
    "print()\n",
    "\n",
    "# === Perform t-tests on absolute values ===\n",
    "comparisons = [(\"Task\", \"Rest\"), (\"Music\", \"Rest\")]\n",
    "results = []\n",
    "\n",
    "for cond1, cond2 in comparisons:\n",
    "    for feat in features_to_compare:\n",
    "        data1 = df_all[df_all[\"Condition\"] == cond1][feat].dropna().abs()\n",
    "        data2 = df_all[df_all[\"Condition\"] == cond2][feat].dropna().abs()\n",
    "        t_stat, p_val = ttest_ind(data1, data2, equal_var=False)\n",
    "        results.append({\n",
    "            \"Comparison\": f\"{cond1} vs. {cond2}\",\n",
    "            \"Feature\": feat,\n",
    "            \"t-value\": round(t_stat, 4),\n",
    "            \"p-value\": round(p_val, 4)\n",
    "        })\n",
    "\n",
    "# Convert to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# === Display ===\n",
    "print(\"=== Statistical Comparison Across All Participants (Absolute Values) ===\")\n",
    "print(results_df)\n",
    "\n",
    "# Save results\n",
    "results_df.to_csv(os.path.join(base_path, \"group_comparison_results_abs.csv\"), index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training binary classifier based on selected features: mlp, svm, snn..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV, learning_curve\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# === Load Combined Features ===\n",
    "df = pd.read_csv(\"/Users/kyrusmama/Downloads/ref_prep_synchr_MAT_jiahui/Data_all/sub-10/features.csv\")#only contain labeled data for sub-10\n",
    "df = df[df[\"Condition\"].isin([\"Task\", \"Rest\"])]  # binary classification only\n",
    "\n",
    "# === Define Features & Labels ===\n",
    "features = [\"Engagement\",  \"Alpha/Theta\", \"Theta/Alpha\",  \"Theta\", \"Alpha\"]\n",
    "\n",
    "# Drop rows with missing features\n",
    "df = df.dropna(subset=features)\n",
    "\n",
    "X = df[features].values\n",
    "y = df[\"Condition\"].map({\"Task\": 1, \"Rest\": 0}).values\n",
    "\n",
    "# === Normalize ===\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# === Grid Search for MLP ===\n",
    "param_grid = {\n",
    "    \"hidden_layer_sizes\": [(50,), (100,), (50, 50), (100, 50)],\n",
    "    \"activation\": [\"relu\", \"tanh\", \"logistic\"],\n",
    "    \"alpha\": [0.0001, 0.001, 0.01],\n",
    "    \"learning_rate_init\": [0.001, 0.01, 0.1],\n",
    "    \"batch_size\": [1, 5, 16],\n",
    "    \"solver\": [\"adam\", \"sgd\"],\n",
    "}\n",
    "\n",
    "mlp = MLPClassifier(max_iter=500, random_state=42)\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "grid_search = GridSearchCV(mlp, param_grid, scoring=\"accuracy\", cv=cv, n_jobs=-1, verbose=2)\n",
    "grid_search.fit(X_scaled, y)\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "print(\"Best MLP Parameters:\", grid_search.best_params_)\n",
    "\n",
    "# === Evaluate with Cross-Validation ===\n",
    "metrics = {\"accuracy\": [], \"precision\": [], \"recall\": [], \"f1\": []}\n",
    "\n",
    "for train_idx, test_idx in cv.split(X_scaled, y):\n",
    "    X_train, X_test = X_scaled[train_idx], X_scaled[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "    best_model.fit(X_train, y_train)\n",
    "    y_pred = best_model.predict(X_test)\n",
    "\n",
    "    metrics[\"accuracy\"].append(accuracy_score(y_test, y_pred))\n",
    "    metrics[\"precision\"].append(precision_score(y_test, y_pred))\n",
    "    metrics[\"recall\"].append(recall_score(y_test, y_pred))\n",
    "    metrics[\"f1\"].append(f1_score(y_test, y_pred))\n",
    "\n",
    "# Print mean ± std\n",
    "print(\"\\n Cross-Validated Performance:\")\n",
    "for m, scores in metrics.items():\n",
    "    print(f\"{m.capitalize()}: {np.mean(scores):.3f} ± {np.std(scores):.3f}\")\n",
    "\n",
    "# === Learning Curve Plot ===\n",
    "train_sizes, train_scores, test_scores = learning_curve(\n",
    "    best_model, X_scaled, y, cv=cv, scoring='accuracy',\n",
    "    train_sizes=np.linspace(0.1, 1.0, 10), n_jobs=-1, shuffle=True, random_state=42\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.title(\"Learning Curve - Best MLP\")\n",
    "plt.plot(train_sizes, train_scores.mean(axis=1), 'o-', label=\"Train\")\n",
    "plt.plot(train_sizes, test_scores.mean(axis=1), 'o-', label=\"Validation\")\n",
    "plt.fill_between(train_sizes, train_scores.mean(axis=1) - train_scores.std(axis=1),\n",
    "                 train_scores.mean(axis=1) + train_scores.std(axis=1), alpha=0.1)\n",
    "plt.fill_between(train_sizes, test_scores.mean(axis=1) - test_scores.std(axis=1),\n",
    "                 test_scores.mean(axis=1) + test_scores.std(axis=1), alpha=0.1)\n",
    "plt.xlabel(\"Training Size\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.grid(True)\n",
    "plt.legend(loc=\"best\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine all participants' features.csv files into one\n",
    "# Base folder where all participant folders are stored\n",
    "import pandas as pd\n",
    "import os\n",
    "all_dfs = []\n",
    "\n",
    "for folder in sorted(os.listdir(base_path)):\n",
    "    sub_path = os.path.join(base_path, folder)\n",
    "    feature_file = os.path.join(sub_path, \"features.csv\")\n",
    "    \n",
    "    if os.path.exists(feature_file):\n",
    "        try:\n",
    "            df = pd.read_csv(feature_file)\n",
    "            df[\"Participant\"] = folder  # add ID\n",
    "            all_dfs.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {feature_file}: {e}\")\n",
    "\n",
    "# Combine all\n",
    "if all_dfs:\n",
    "    df_all = pd.concat(all_dfs, ignore_index=True)\n",
    "    output_path = os.path.join(base_path, \"all_participants_features.csv\")\n",
    "    df_all.to_csv(output_path, index=False)\n",
    "    print(f\" Combined CSV saved to: {output_path}\")\n",
    "else:\n",
    "    print(\" No features.csv found in any subfolder.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only with sub-10 as example\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV, learning_curve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -------------------------------\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# -------------------------------\n",
    "# SVM Training & Evaluation\n",
    "# -------------------------------\n",
    "def tune_and_evaluate_svm(X, y, kernel):\n",
    "    # Define parameter grid\n",
    "    if kernel == \"linear\":\n",
    "        param_grid = {'C': [0.01, 0.1, 1, 10, 20,50,100]}\n",
    "    elif kernel == \"poly\":\n",
    "        param_grid = {'C': [0.01, 0.1, 1, 10,100,200,500], 'degree': [2, 3, 4,5,6], 'coef0': [0.0, 0.5, 1.0]}\n",
    "    elif kernel == \"rbf\":\n",
    "        param_grid = {'C': [0.01, 0.1, 1, 10,100,200,500], 'gamma': ['scale', 'auto', 0.01, 0.1, 1]}\n",
    "\n",
    "    model = SVC(kernel=kernel, probability=True, random_state=42)\n",
    "    grid_search = GridSearchCV(model, param_grid, scoring='accuracy',\n",
    "                               cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
    "                               n_jobs=-1)\n",
    "\n",
    "    grid_search.fit(X, y)\n",
    "    best_model = grid_search.best_estimator_\n",
    "    best_params = grid_search.best_params_\n",
    "\n",
    "    # Cross-validated metrics\n",
    "    metrics = {'accuracy': [], 'precision': [], 'recall': [], 'f1': []}\n",
    "    kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    for train_idx, test_idx in kf.split(X, y):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "        best_model.fit(X_train, y_train)\n",
    "        y_pred = best_model.predict(X_test)\n",
    "\n",
    "        metrics['accuracy'].append(accuracy_score(y_test, y_pred))\n",
    "        metrics['precision'].append(precision_score(y_test, y_pred, average=\"macro\", zero_division=1))\n",
    "        metrics['recall'].append(recall_score(y_test, y_pred, average=\"macro\", zero_division=1))\n",
    "        metrics['f1'].append(f1_score(y_test, y_pred, average=\"macro\", zero_division=1))\n",
    "\n",
    "    final_metrics = {k: (np.mean(v), np.std(v)) for k, v in metrics.items()}\n",
    "    return best_model, best_params, final_metrics\n",
    "\n",
    "# -------------------------------\n",
    "# Learning Curve Plotter\n",
    "# -------------------------------\n",
    "def plot_learning_curve(model, X, y, title):\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        model, X, y, cv=5, scoring='accuracy', n_jobs=-1,\n",
    "        train_sizes=np.linspace(0.1, 1.0, 10), shuffle=True, random_state=42\n",
    "    )\n",
    "\n",
    "    train_mean, train_std = train_scores.mean(axis=1), train_scores.std(axis=1)\n",
    "    test_mean, test_std = test_scores.mean(axis=1), test_scores.std(axis=1)\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Training Set Size\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.grid(True)\n",
    "    plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1)\n",
    "    plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, alpha=0.1)\n",
    "    plt.plot(train_sizes, train_mean, 'o-', label=\"Training Score\")\n",
    "    plt.plot(train_sizes, test_mean, 'o-', label=\"Validation Score\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# -------------------------------\n",
    "# Run for Each Kernel\n",
    "# -------------------------------\n",
    "for kernel in [\"linear\", \"poly\", \"rbf\"]:\n",
    "    print(f\"\\n**SVM ({kernel} kernel)**\")\n",
    "    best_model, best_params, metrics = tune_and_evaluate_svm(X_scaled, y, kernel)\n",
    "    print(\"Best Parameters:\", best_params)\n",
    "    for m, (mean, std) in metrics.items():\n",
    "        print(f\"{m.capitalize()}: {mean:.3f} ± {std:.3f}\")\n",
    "    plot_learning_curve(best_model, X_scaled, y, title=f\"Learning Curve - SVM ({kernel} kernel)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
